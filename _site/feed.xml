<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-04T15:50:34+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Luca Grementieri</title><subtitle>Scientific blog about maths and machine learning.</subtitle><entry><title type="html">Why convolutions are everywhere</title><link href="http://localhost:4000/2020-03-19/convolutions" rel="alternate" type="text/html" title="Why convolutions are everywhere" /><published>2020-03-19T00:00:00+01:00</published><updated>2020-03-19T00:00:00+01:00</updated><id>http://localhost:4000/2020-03-19/convolutions</id><content type="html" xml:base="http://localhost:4000/2020-03-19/convolutions">&lt;header&gt;
  &lt;h1&gt;Why convolutions are everywhere&lt;/h1&gt;
  &lt;p&gt;
    Modern deep learning architectures often include convolutions. Surely, convolutions are parameter efficient and optimized, but the reason why they are so widespread is much more profound. We are going to show that convolutions arise from simple assumptions on data.
  &lt;/p&gt;
&lt;/header&gt;
&lt;section&gt;
  &lt;h1 id=&quot;signals&quot;&gt;Signals&lt;/h1&gt;
  &lt;p&gt;
    There are many kinds of data: images, sounds, text, time series are very well-known examples. All these data are discrete. In some cases, this property is intrinsic: for example, written text is naturally a sequence of characters and words. In most cases, when data are the result of a measurement, analogic signals are digitized during the acquisition process.
  &lt;/p&gt;
  &lt;p&gt;
    The devices that perform the measurements are called sensors. Sensors produce discrete signals by &lt;em&gt;sampling&lt;/em&gt; or &lt;em&gt;aggregating&lt;/em&gt; information from the analog world around us.
    For example, a sound recorder &lt;em&gt;samples&lt;/em&gt; audio waves to register a discretized version of them. On the other hand, a camera collects photons in every pixel &lt;em&gt;aggregating&lt;/em&gt; the light coming from all directions in a short amount of time.
  &lt;/p&gt;
  &lt;p&gt;
    From the mathematical point of view, a signal is any function defined on a discrete set. This definition is very general, and it includes all the examples above.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A discrete &lt;strong&gt;signal&lt;/strong&gt; is a function whose domain is $\mathbb{Z}$,
    $f: \mathbb{Z} \longrightarrow \mathbb{R}$.&lt;/br&gt;
    We denote the $n$-th value of the function as $f[n]$.
    &lt;small&gt;
      The definition provided is very similar to the one of a sequence. We will use this parallelism to derive some theorems and properties of signals.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
      A very particular signal is the discrete Dirac delta.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    The discrete &lt;strong&gt;Dirac delta&lt;/strong&gt; is the signal
    $\delta[n] =
    \begin{cases}
    1 &amp; \text{if } n = 0 \\
    0 &amp; \text{if } n \neq 0
    \end{cases}$.
  &lt;/p&gt;
  &lt;!--Draw similarity with sequences and speakc about l^p --&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;discrete-convolution&quot;&gt;Discrete convolution&lt;/h1&gt;
  &lt;p&gt;
    The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real signals then the &lt;strong&gt;discrete convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k].
    $$
  &lt;/p&gt;
  &lt;p&gt;
    The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$.
    When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$.
    &lt;small&gt;
      The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: &lt;/br&gt;
      $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    In general, the summation in the convolution definition is not always finite.
    Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals
    with $p, q \in [1, \infty]$ such that
    $$
    \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\},
    $$
    then $f * g$ is bounded.
    &lt;small&gt;
      For every $p &gt; 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences,
      i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if
      $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p}&lt; +\infty$.&lt;/br&gt;
      $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that
    $$
    |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}.
    $$
    Using Hölder's inequality and with a simple change of variable, we prove the theorem:
    &lt;small&gt;
    Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$
    if $p$ and $q$ satisfy the condition of the theorem.
    The special case $p=q=2$ gives the Cauchy–Schwarz inequality.
    &lt;/small&gt;
    $$
    \begin{aligned}
        |(f*g)[n]| &amp; \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\
        &amp; \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} &amp; \textit{\footnotesize Hölder's ineq.} \\
        &amp; = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} &amp; {\footnotesize k' = n - k } \\
        &amp; = ||f||_{p}||g||_{q}.
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p&gt;
    Two remarkable properties of the convolution are commutativity and linearity.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g$ be two real signals such that its convolution is well-defined, then
    $$
    (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    That means that the convolution is a &lt;strong&gt;commutative&lt;/strong&gt; operation.
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$.
    $$
    \begin{aligned}
    (f * g)[n] &amp; = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\
    &amp; = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n].
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below
    are all well-defined, then we have
    $$
    ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    Thus the convolution is a &lt;strong&gt;linear&lt;/strong&gt; operation.
  &lt;/p&gt;
  &lt;p&gt;
    These two properties are sufficient to explain why convolutions are a fundamental component
    of many processing systems.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators&quot;&gt;Operators&lt;/h1&gt;
    &lt;p&gt;
        A signal can be analyzed and manipulated by operators.
        For example we can use an operator to remove or reduce noise.
        A resampling or an approximation are other examples of manipulations that are associated
        to an operator. The same can be told for the discrete differentiation and integration.
        Every operation applied to a signal is associated to an operator.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is &lt;strong&gt;linear&lt;/strong&gt; if $\forall\: a, b \in \mathbb{R}$
      $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator is &lt;strong&gt;shift-invariant&lt;/strong&gt; if $\forall\: p \in \mathbb{Z}$
      $$L(f[n-p]) = L(f)[n-p].$$
    &lt;/p&gt;
    &lt;p&gt;
        The shift invariance assures that if the signal is shifted even the result of the operator
        is shifted. Finite differences are examples of shift-invariant operators.
        On the other hand, operations executed on a sliding window over the signal can be shift-invariant
        or not. It depends on the stride of the sliding window, i.e. the difference between
        two consecutive positions of the window.
        If the stride is 1, then the operator is shift-invariant, otherwise it is not.
    &lt;/p&gt;
    &lt;p&gt;
        Another interpretation of shift-invariance is that the result of the operation doesn't depend
        on the absolute position of elements in a signal, but just on their relative position.
        This new perspective on this property explains why it is of paramount importance.
        Many analysis on data should not depend on the absolute position of the values in the signal,
        in fact in most situations this position cannot be carefully chosen.
        For example when we analyze an image we do not want to rely on the absolute position of
        pixels because those positions change as soon as the image is cut or resized.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators-convolution&quot;&gt;Operators and convolution&lt;/h1&gt;
    &lt;p&gt;
        There is a tight relationship between linear shift-invariant operators and convolution.
        In order to show this relationship we need to recall the discrete Dirac.
        In particular we can notice that translating the discrete Dirac by $p$ we obtain
        $$
        \delta[n - p] =
        \begin{cases}
        1 &amp; n = p \\
        0 &amp; n \neq p
        \end{cases}.
        $$
    &lt;/p&gt;
    &lt;p&gt;
        Using this formulation we can express every signal as a linear combination of Dirac signals
        $$
        f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
        $$
    &lt;/p&gt;
    &lt;p&gt;
        With this fact in mind we can enunciate and prove the main theorem of this post.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    &lt;/p&gt;
    &lt;h3&gt;Proof&lt;/h3&gt;
    &lt;p&gt;
        If $L$ is linear and shift-invariant we set $h[n] = L(\delta)[n]$.
        Thus
        $$
        \begin{aligned}
        L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp;
        \end{aligned}
        $$
        To prove the inverse implication we have to show that $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution proved above.
        We show here that the resulting operation is also shift invariant.
        $$
        \begin{aligned}
            L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\
            &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\
            &amp; = L(f)[n - p]
        \end{aligned}
        $$
    &lt;/p&gt;
    &lt;p&gt;
        The theorem proved shows that convolution are used in many applications because there is
        no alternative to express linear shift-invariant operations.
        We have already discussed why the shift-invariance is often a required property, the linearity is often
        requested because it simplifies a lot the operation and most basic operators are linear.
    &lt;/p&gt;
    &lt;p&gt;
        Sometimes the linearity is required by properties of the data too.
        &lt;!-- add reference --&gt;
        For example sound waves satisfy the superposition property, i.e. the result of the superposition
        of two waves can be obtained summing up the single waves.
        In such cases it's natural to use linear operators because the linearity assures the
        validity of the superposition principle also for the resulting waves.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;generalizations&quot;&gt;Generalizations&lt;/h1&gt;
  &lt;p&gt;
    In mathematics the convolution operation is usually introduced as an operation of two functions.
    On the other hand, in deep learning convolution is an operation of discrete signals.
    To understand the properties of this operation we are going to study the operation in the continuous world
    and then define an equivalent discrete version that can be used in practice on real data.
    In the beginning we are going to restrict our analysis to 1D functions, then we will generalize it to
    multiple dimensions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real function then the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt.
    $$
  &lt;/p&gt;
  &lt;p&gt;
    The integral in the convolution definition is not always defined but
    recalling Holder's inequality we can find
    a sufficient condition to establish its existence.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f \in L^{p}(\mathbb{R})$ and $g \in L^{q}(\mathbb{R})$ be functions
    with $p, q \in [1, \infty]$ such that
    $$
    \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\},
    $$
    then $f * g$ is bounded.
    &lt;small&gt;
        $L^{\infty}(\mathbb{R})$ is the space of functions essentially bounded, i.e.
        $f \in L^{\infty}(\mathbb{R})$ if and only if $\exists \:M \in \mathbb{R}$
        such that $\mu(\{x \in \mathbb{R}, |f(x)| \geq M\}) = 0$.
        $$
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    $f * g$ is a bounded function if $\exist \:M \in \mathbb{R}$ such that
    $$
    |(f*g)(x)| \leq M \quad \forall\:x \in \mathbb{R}.
    $$
    Using Hölder's inequality and with a simple change of variable we conclude the proof
    &lt;small&gt;
    Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$
    if $p$, $q$ satisfies the condition of the theorem.
    The special case $p=q=2$ gives the Cauchy–Schwarz inequality.
    &lt;/small&gt;
    $$
    \begin{aligned}
        |(f*g)(x)| &amp; \leq \int_{-\infty}^{+\infty}|f(t)g(x-t)| dt \\
        &amp; \leq \left[\int_{-\infty}^{+\infty}|f(t)|^{p}dt\right]^{\frac{1}{p}}
        \left[\int_{-\infty}^{+\infty}|g(x-t)|^{q}dt\right]^{\frac{1}{q}} \\
        &amp; = \left[\int_{-\infty}^{+\infty}|f(t)|^{p}dt\right]^{\frac{1}{p}}
        \left[\int_{-\infty}^{+\infty}|g(t)|^{q}dt\right]^{\frac{1}{q}} \\
        &amp; = ||f||_{p}||g||_{q}.
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p&gt;
      Another remarkable property of the convolution is the commutativity.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g$ two real functions such that its convolution is well-defined, then
    $$
    (f * g)(x) = (g * f)(x) \quad \forall\: x \in \mathbb{R}.
    $$
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    For any fixed $x \in \mathbb{R}$, let $v = x-t$. Then $t = x- v$ and
    $$
    \begin{aligned}
    (f * g)(x) &amp; = \int_{-\infty}^{+\infty}f(t)g(x-t) dt \\
    &amp; = \int_{+\infty}^{-\infty}-f(x-v)g(v) dv \\
    &amp; = \int_{-\infty}^{+\infty}g(v)f(x-v) dv = (g * f)(x).
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p&gt;
    These properties are sufficient to explain why convolutions are a fundamental component
    of every processing system.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;cnn&quot;&gt;Convolutional Neural Networks&lt;/h1&gt;
    &lt;p&gt;
        The convolution operation has become a fundamental operation in the context
        of neural networks, in particolar in the field of computer vision.
        In fact, Convolutional Neural Networks (CNN) are a sequence of 3D convolutions followed
        by pooling operations.
    &lt;/p&gt;
    &lt;p&gt;
        The most common pooling operations are max-pooling and average-pooling.
        In both cases a 2D sliding window, usually of size $2 \times 2$ is passed over the output of the convolution.
        As we have noticed after the defintion of shift-invariance, operations over a sliding windows
        are shift-invariant only if the stride of the sliding window is 1.
    &lt;/p&gt;
    &lt;p&gt;
        Currently most successful CNN rely on pooling operation with stride 2.
        So they do not describe shift-invariant operations!
        This fact has been noticed by some researchers that have proposed alternative pooling layers
        as replacement to correct this flaw.
        The correction of this defect gives CNNs more robust to noise and less prone to adversarial attacks.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
    &lt;p&gt;
        The convolution operation doesn't come out-of-the-blue.
        It can be deduced from simple principles and assumptions on the data under exam.
        The knowledge of these assumptions is fundamental to drive machine learning
        researchers and practitioners in adapting state-of-the-art algorithms to different data and problems.
    &lt;/p&gt;
&lt;/section&gt;
&lt;footer&gt;
  &lt;section&gt;
    &lt;h1&gt;References&lt;/h1&gt;
  &lt;/section&gt;
  &lt;section&gt;
    &lt;h1&gt;Updates and Corrections&lt;/h1&gt;
    &lt;p&gt;If you see mistakes or want to suggest changes, please &lt;a href=&quot;https://github.com/nextbitlabs/Rapido/issues&quot;&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt;
  &lt;/section&gt;
&lt;/footer&gt;</content><author><name></name></author><summary type="html">Why convolutions are everywhere Modern deep learning architectures often include convolutions. Surely, convolutions are parameter efficient and optimized, but the reason why they are so widespread is much more profound. We are going to show that convolutions arise from simple assumptions on data. Signals There are many kinds of data: images, sounds, text, time series are very well-known examples. All these data are discrete. In some cases, this property is intrinsic: for example, written text is naturally a sequence of characters and words. In most cases, when data are the result of a measurement, analogic signals are digitized during the acquisition process. The devices that perform the measurements are called sensors. Sensors produce discrete signals by sampling or aggregating information from the analog world around us. For example, a sound recorder samples audio waves to register a discretized version of them. On the other hand, a camera collects photons in every pixel aggregating the light coming from all directions in a short amount of time. From the mathematical point of view, a signal is any function defined on a discrete set. This definition is very general, and it includes all the examples above. DEFINITION A discrete signal is a function whose domain is $\mathbb{Z}$, $f: \mathbb{Z} \longrightarrow \mathbb{R}$. We denote the $n$-th value of the function as $f[n]$. The definition provided is very similar to the one of a sequence. We will use this parallelism to derive some theorems and properties of signals. A very particular signal is the discrete Dirac delta. DEFINITION The discrete Dirac delta is the signal $\delta[n] = \begin{cases} 1 &amp; \text{if } n = 0 \\ 0 &amp; \text{if } n \neq 0 \end{cases}$. Discrete convolution The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions. DEFINITION Let $f$, $g$ be two real signals then the discrete convolution of $f$ and $g$ is $$ (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k]. $$ The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$. When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$. The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$. In general, the summation in the convolution definition is not always finite. Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element. THEOREM Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals with $p, q \in [1, \infty]$ such that $$ \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\}, $$ then $f * g$ is bounded. For every $p &gt; 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences, i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p} $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences. Proof $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that $$ |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}. $$ Using Hölder's inequality and with a simple change of variable, we prove the theorem: Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$ if $p$ and $q$ satisfy the condition of the theorem. The special case $p=q=2$ gives the Cauchy–Schwarz inequality. $$ \begin{aligned} |(f*g)[n]| &amp; \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\ &amp; \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}} \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} &amp; \textit{\footnotesize Hölder's ineq.} \\ &amp; = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}} \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} &amp; {\footnotesize k' = n - k } \\ &amp; = ||f||_{p}||g||_{q}. \end{aligned} $$ Two remarkable properties of the convolution are commutativity and linearity. THEOREM Let $f, g$ be two real signals such that its convolution is well-defined, then $$ (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}. $$ That means that the convolution is a commutative operation. Proof For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$. $$ \begin{aligned} (f * g)[n] &amp; = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\ &amp; = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n]. \end{aligned} $$ THEOREM Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below are all well-defined, then we have $$ ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}. $$ Thus the convolution is a linear operation. These two properties are sufficient to explain why convolutions are a fundamental component of many processing systems. Operators A signal can be analyzed and manipulated by operators. For example we can use an operator to remove or reduce noise. A resampling or an approximation are other examples of manipulations that are associated to an operator. The same can be told for the discrete differentiation and integration. Every operation applied to a signal is associated to an operator. DEFINITION A discrete operator $L$ is linear if $\forall\: a, b \in \mathbb{R}$ $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$ DEFINITION A discrete operator is shift-invariant if $\forall\: p \in \mathbb{Z}$ $$L(f[n-p]) = L(f)[n-p].$$ The shift invariance assures that if the signal is shifted even the result of the operator is shifted. Finite differences are examples of shift-invariant operators. On the other hand, operations executed on a sliding window over the signal can be shift-invariant or not. It depends on the stride of the sliding window, i.e. the difference between two consecutive positions of the window. If the stride is 1, then the operator is shift-invariant, otherwise it is not. Another interpretation of shift-invariance is that the result of the operation doesn't depend on the absolute position of elements in a signal, but just on their relative position. This new perspective on this property explains why it is of paramount importance. Many analysis on data should not depend on the absolute position of the values in the signal, in fact in most situations this position cannot be carefully chosen. For example when we analyze an image we do not want to rely on the absolute position of pixels because those positions change as soon as the image is cut or resized. Operators and convolution There is a tight relationship between linear shift-invariant operators and convolution. In order to show this relationship we need to recall the discrete Dirac. In particular we can notice that translating the discrete Dirac by $p$ we obtain $$ \delta[n - p] = \begin{cases} 1 &amp; n = p \\ 0 &amp; n \neq p \end{cases}. $$ Using this formulation we can express every signal as a linear combination of Dirac signals $$ f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]. $$ With this fact in mind we can enunciate and prove the main theorem of this post. THEOREM A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete signal $h[n]$ such that for every signal $f[n]$ $$ L(f)[n] = (f * h)[n]. $$ Proof If $L$ is linear and shift-invariant we set $h[n] = L(\delta)[n]$. Thus $$ \begin{aligned} L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp; \end{aligned} $$ To prove the inverse implication we have to show that $L(f) = f * h$ is a linear shift-invariant operator. The linearity comes from the linearity of the convolution proved above. We show here that the resulting operation is also shift invariant. $$ \begin{aligned} L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\ &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\ &amp; = L(f)[n - p] \end{aligned} $$ The theorem proved shows that convolution are used in many applications because there is no alternative to express linear shift-invariant operations. We have already discussed why the shift-invariance is often a required property, the linearity is often requested because it simplifies a lot the operation and most basic operators are linear. Sometimes the linearity is required by properties of the data too. For example sound waves satisfy the superposition property, i.e. the result of the superposition of two waves can be obtained summing up the single waves. In such cases it's natural to use linear operators because the linearity assures the validity of the superposition principle also for the resulting waves. Generalizations In mathematics the convolution operation is usually introduced as an operation of two functions. On the other hand, in deep learning convolution is an operation of discrete signals. To understand the properties of this operation we are going to study the operation in the continuous world and then define an equivalent discrete version that can be used in practice on real data. In the beginning we are going to restrict our analysis to 1D functions, then we will generalize it to multiple dimensions. DEFINITION Let $f$, $g$ be two real function then the convolution of $f$ and $g$ is $$ (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt. $$ The integral in the convolution definition is not always defined but recalling Holder's inequality we can find a sufficient condition to establish its existence. THEOREM Let $f \in L^{p}(\mathbb{R})$ and $g \in L^{q}(\mathbb{R})$ be functions with $p, q \in [1, \infty]$ such that $$ \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\}, $$ then $f * g$ is bounded. $L^{\infty}(\mathbb{R})$ is the space of functions essentially bounded, i.e. $f \in L^{\infty}(\mathbb{R})$ if and only if $\exists \:M \in \mathbb{R}$ such that $\mu(\{x \in \mathbb{R}, |f(x)| \geq M\}) = 0$. $$ Proof $f * g$ is a bounded function if $\exist \:M \in \mathbb{R}$ such that $$ |(f*g)(x)| \leq M \quad \forall\:x \in \mathbb{R}. $$ Using Hölder's inequality and with a simple change of variable we conclude the proof Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$ if $p$, $q$ satisfies the condition of the theorem. The special case $p=q=2$ gives the Cauchy–Schwarz inequality. $$ \begin{aligned} |(f*g)(x)| &amp; \leq \int_{-\infty}^{+\infty}|f(t)g(x-t)| dt \\ &amp; \leq \left[\int_{-\infty}^{+\infty}|f(t)|^{p}dt\right]^{\frac{1}{p}} \left[\int_{-\infty}^{+\infty}|g(x-t)|^{q}dt\right]^{\frac{1}{q}} \\ &amp; = \left[\int_{-\infty}^{+\infty}|f(t)|^{p}dt\right]^{\frac{1}{p}} \left[\int_{-\infty}^{+\infty}|g(t)|^{q}dt\right]^{\frac{1}{q}} \\ &amp; = ||f||_{p}||g||_{q}. \end{aligned} $$ Another remarkable property of the convolution is the commutativity. THEOREM Let $f, g$ two real functions such that its convolution is well-defined, then $$ (f * g)(x) = (g * f)(x) \quad \forall\: x \in \mathbb{R}. $$ Proof For any fixed $x \in \mathbb{R}$, let $v = x-t$. Then $t = x- v$ and $$ \begin{aligned} (f * g)(x) &amp; = \int_{-\infty}^{+\infty}f(t)g(x-t) dt \\ &amp; = \int_{+\infty}^{-\infty}-f(x-v)g(v) dv \\ &amp; = \int_{-\infty}^{+\infty}g(v)f(x-v) dv = (g * f)(x). \end{aligned} $$ These properties are sufficient to explain why convolutions are a fundamental component of every processing system. Convolutional Neural Networks The convolution operation has become a fundamental operation in the context of neural networks, in particolar in the field of computer vision. In fact, Convolutional Neural Networks (CNN) are a sequence of 3D convolutions followed by pooling operations. The most common pooling operations are max-pooling and average-pooling. In both cases a 2D sliding window, usually of size $2 \times 2$ is passed over the output of the convolution. As we have noticed after the defintion of shift-invariance, operations over a sliding windows are shift-invariant only if the stride of the sliding window is 1. Currently most successful CNN rely on pooling operation with stride 2. So they do not describe shift-invariant operations! This fact has been noticed by some researchers that have proposed alternative pooling layers as replacement to correct this flaw. The correction of this defect gives CNNs more robust to noise and less prone to adversarial attacks. Conclusions The convolution operation doesn't come out-of-the-blue. It can be deduced from simple principles and assumptions on the data under exam. The knowledge of these assumptions is fundamental to drive machine learning researchers and practitioners in adapting state-of-the-art algorithms to different data and problems. References Updates and Corrections If you see mistakes or want to suggest changes, please open an issue on GitHub.</summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/2020-03-18/welcome-to-jekyll" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2020-03-18T22:49:49+01:00</published><updated>2020-03-18T22:49:49+01:00</updated><id>http://localhost:4000/2020-03-18/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/2020-03-18/welcome-to-jekyll">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;Jekyll requires blog post files to be named according to the following format:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR-MONTH-DAY-title.MARKUP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR&lt;/code&gt; is a four-digit number, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MONTH&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAY&lt;/code&gt; are both two-digit numbers, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MARKUP&lt;/code&gt; is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry></feed>