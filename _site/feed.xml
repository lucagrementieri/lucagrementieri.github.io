<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-19T11:47:38+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Luca Grementieri</title><subtitle>Scientific blog about maths and machine learning</subtitle><author><name>Luca Grementieri</name></author><entry><title type="html">Artificial neurons as ridge functions</title><link href="http://localhost:4000/2020-04-13/ridge" rel="alternate" type="text/html" title="Artificial neurons as ridge functions" /><published>2020-04-13T00:00:00+02:00</published><updated>2020-04-13T00:00:00+02:00</updated><id>http://localhost:4000/2020-04-13/ridge</id><content type="html" xml:base="http://localhost:4000/2020-04-13/ridge">&lt;!-- improve intro --&gt;
$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}
\gdef\a{\bold{a}}

\gdef\R{\mathbb{R}}
\gdef\Rn{\mathbb{R}^{n}}
\gdef\M{\mathcal{M}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
$
&lt;section&gt;
  &lt;h1 id=&quot;approximation-theory&quot;&gt;Approximation theory&lt;/h1&gt;
  &lt;p&gt;
    Approximation theory is the branch of mathematics that studies how to approximate a class of functions
    with a set of simpler functions. It allows to discover set of functions described by finite or countable
    parameters that can faithfully represent an uncountable class of functions.
    This subject has great practical interest because it allows to solve optimization problems on functions without making strong assumptions on the searched function.
  &lt;/p&gt;
  &lt;p&gt;
    Given a functional $\ell: \mathcal{F} \longrightarrow \R$,
    an optimization problem takes the form
    $$
    f^{*} = \argmin{f \in \mathcal{F}}\ \ell(f),
    $$
    where $\mathcal{F}$ is the search space of functions determined by the assumptions on $f^{*}$.
    For example, if we know that the solution $f^{*}$ is a continuous univariate function, then $\mathcal{F} = C(\R)$.
  &lt;/p&gt;
  &lt;p&gt;
    In practice we cannot solve the optimization problem on a function space with an infinite number of dimensions,
    because machines can work only with a finite number of parameters. So we need to restrict $\mathcal{F}$ to a more
    manageable subset $\mathcal{G} \subset \mathcal{F}$.
    This restriction adds more assumptions on $f^{*}$, assumptions not dictated by the problem itself but by
    computational requirements. For example, in many cases, the search space is restricted to linear functions.
  &lt;/p&gt;
  &lt;p&gt;
    Approximation theory looks for a countable or uncountable $\mathcal{G}$ dense in $\mathcal{F}$.
    The density property assures that the effective search space is not reduced because $f^{*}$ can be described
    by functions in $\mathcal{G}$ with arbitrary precision.
    Let us recall the definition of density.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $(X, d)$ be a metric space and let $S \subseteq X$. $S$ is &lt;strong&gt;dense&lt;/strong&gt; in $X$
    if for every $x \in X$ and every $\epsilon &gt; 0$, there exist $s \in S$ such that $d(x, s) &lt; \epsilon$.
    Equivalently, $S$ is dense in $X$ if the closure of $S$ is $X$: $\overline{S} = X$.
  &lt;/p&gt;
  &lt;p&gt;
    The most known result of approximation theory is Weierstrass Approximation Theorem.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f$ be a continuous real-valued function defined on the interval $[a, b]$.
    For every $\epsilon &gt; 0$, there exists a polynomial $p$ such that $\forall\, x \in [a, b]$, we have
    $|f(x) − p(x)| &lt; \epsilon$, or equivalently, the supremum norm $||f−p|| &lt; \epsilon$.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;mlp&quot;&gt;Multilayer perceptron&lt;/h1&gt;
  &lt;p&gt;
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of &lt;em&gt;layers&lt;/em&gt;.
    Every layer is composed by &lt;em&gt;neurons&lt;/em&gt;, the fundamental processing unit of the network.
    Every artificial neuron in the intermediate &lt;em&gt;hidden layers&lt;/em&gt; processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the &lt;em&gt;activation function&lt;/em&gt; $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    &lt;small&gt;
      In the neural network literature, the values $w_{i}$ are called &lt;em&gt;weights&lt;/em&gt; and they model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron. $\theta$ is called &lt;em&gt;bias&lt;/em&gt; and it recalls the threshold potential involved in the firing of biologic neurons.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    In the final layer, said the &lt;em&gt;output layer&lt;/em&gt;, neurons operates differently. They just perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this model because it can already approximate a very large class of functions.
    &lt;!-- add small to explain that we are dealing with regression but classification is shaped as a regression too--&gt;
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;ridge&quot;&gt;Ridge functions&lt;/h1&gt;
  &lt;p&gt;
    The function $N(\x)$ has a particular form: it is the composition of a univariate function $\sigma$ with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A &lt;strong&gt;ridge function&lt;/strong&gt; is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  &lt;/p&gt;
  &lt;p&gt;
    The vector $\a$ is called the &lt;em&gt;direction&lt;/em&gt; because a ridge function is a multivariate function constant on the parallel hyperplanes orthogonal to $\a$. In fact, these hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  &lt;/p&gt;
  &lt;p&gt;
    For a given direction $\a$, we denote the set of ridge functions with direction $\a$
    $$\M(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\M(\a) = \M(\bold{b})$.
  &lt;/p&gt;
  &lt;p&gt;
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \M(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A &lt;em&gt;span&lt;/em&gt; of a set $S$ is the set of linear combinations of elements of $S$, then
    every $F \in \M(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\M(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
    We are interested in the theory of approximation of sets $\M(\Omega)$ to better characterize the class of functions defined by MLPs.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators&quot;&gt;Operators&lt;/h1&gt;
    &lt;p&gt;
        To extract information from a stream of data, we have to process it.
        Every operation applied to a signal is associated with an operator.
        For example, we can use operators to reduce noise, to recognize features,
        to detect peaks or discontinuities.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is &lt;strong&gt;linear&lt;/strong&gt; if $\forall\: a, b \in \mathbb{R}$
      $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator is &lt;strong&gt;shift-invariant&lt;/strong&gt; if $\forall\: p \in \mathbb{Z}$
      $$L(f[n-p]) = L(f)[n-p].$$
    &lt;/p&gt;
    &lt;p&gt;
      When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant.
      The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters.
    &lt;/p&gt;
    &lt;p&gt;
      This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators-convolution&quot;&gt;Operators and convolution&lt;/h1&gt;
    &lt;p&gt;
      There is a tight relationship between linear shift-invariant operators and convolution.
      To reveal this connection, we need to recall the discrete Dirac.
      Translating the discrete Dirac by $p$, we obtain
      $$
      \delta[n - p] =
      \begin{cases}
      1 &amp; n = p \\
      0 &amp; n \neq p
      \end{cases}.
      $$
      Using this expression, we can represent every signal as a linear combination of Dirac signals
      $$
      f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
      $$
      Now we have all the elements necessary to prove the main theorem of the article.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    &lt;/p&gt;
    &lt;h3&gt;Proof&lt;/h3&gt;
    &lt;p&gt;
        If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$.
        Setting $h[n] = L(\delta)[n]$, we have
        $$
        \begin{aligned}
        L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp;
        \end{aligned}
        $$
        To prove the inverse implication, we have to show that for every signal $h[n]$,
        $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution.
        Now, we show that the resulting operation is also shift-invariant.
        $$
        \begin{aligned}
            L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\
            &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\
            &amp; = L(f)[n - p]. &amp; \square
        \end{aligned}
        $$
    &lt;/p&gt;
    &lt;p&gt;
      This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations.
      We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons.
      Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines.
    &lt;/p&gt;
    &lt;p&gt;
      Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;generalizations&quot;&gt;Generalizations&lt;/h1&gt;
  &lt;p&gt;
    In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real function, the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt  \quad \forall\: x \in \mathbb{R}.
    $$
  &lt;/p&gt;
  &lt;p&gt;
    All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions.
    &lt;small&gt;
      Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension.
    We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals,
    the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k]  \quad \forall\: n \in \mathbb{Z}^{d}.
    $$
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;cnn&quot;&gt;Convolutional Neural Networks&lt;/h1&gt;
    &lt;p&gt;
      The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision.
      A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling.
    &lt;/p&gt;
    &lt;p&gt;
      Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant.
    &lt;/p&gt;
    &lt;p&gt;
      Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen.
    &lt;/p&gt;
    &lt;p&gt;
      Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation!
      A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper &quot;&lt;a href=https://arxiv.org/abs/1904.11486&gt;Making Convolutional Networks Shift-Invariant Again&lt;/a&gt;&quot;.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
    &lt;p&gt;
      The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications.
    &lt;/p&gt;
&lt;/section&gt;
&lt;footer style=&quot;text-align: left&quot;&gt;
  &lt;section&gt;
    &lt;h1&gt;References&lt;/h1&gt;
    &lt;ol&gt;
      &lt;li id=&quot;ref-1&quot;&gt;
        Mallat, Stéphane. &lt;em&gt;A wavelet tour of signal processing&lt;/em&gt;. Elsevier, 1999.
      &lt;/li&gt;
      &lt;li id=&quot;ref-2&quot;&gt;
        Zhang, Richard. &lt;em&gt;Making convolutional networks shift-invariant again&lt;/em&gt;. arXiv preprint arXiv:1904.11486 (2019).
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/section&gt;
  &lt;section&gt;
    &lt;h1&gt;Updates and Corrections&lt;/h1&gt;
    &lt;p&gt;If you see mistakes or want to suggest changes, please &lt;a href=&quot;https://github.com/lucagrementieri/lucagrementieri.github.io&quot;&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt;
  &lt;/section&gt;
&lt;/footer&gt;</content><author><name>Luca Grementieri</name></author><summary type="html">$ \gdef\x{\bold{x}} \gdef\w{\bold{w}} \gdef\a{\bold{a}}</summary></entry><entry><title type="html">Why convolutions are everywhere</title><link href="http://localhost:4000/2020-04-05/convolutions" rel="alternate" type="text/html" title="Why convolutions are everywhere" /><published>2020-04-05T00:00:00+02:00</published><updated>2020-04-05T00:00:00+02:00</updated><id>http://localhost:4000/2020-04-05/convolutions</id><content type="html" xml:base="http://localhost:4000/2020-04-05/convolutions">&lt;section&gt;
  &lt;h1 id=&quot;signals&quot;&gt;Signals&lt;/h1&gt;
  &lt;p&gt;
    There are many kinds of data: images, sounds, text, time series are very well-known examples. All these data are discrete. In some cases, this property is intrinsic: for example, written text is naturally a sequence of characters and words. In most cases, when data are the result of a measurement, analog signals are digitized during the acquisition process.
  &lt;/p&gt;
  &lt;p&gt;
    The devices that perform the measurements are called sensors. Sensors produce discrete signals by &lt;em&gt;sampling&lt;/em&gt; or &lt;em&gt;aggregating&lt;/em&gt; information from the analog world around us.
    For example, a sound recorder &lt;em&gt;samples&lt;/em&gt; audio waves to register a discretized version of them. On the other hand, a camera collects photons in every pixel &lt;em&gt;aggregating&lt;/em&gt; the light coming from all directions in a short amount of time.
  &lt;/p&gt;
  &lt;p&gt;
    From a mathematical point of view, a signal is any function defined on a discrete set. This definition is very general, and it includes all the examples above.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A discrete &lt;strong&gt;signal&lt;/strong&gt; is a function whose domain is $\mathbb{Z}$,
    $f: \mathbb{Z} \longrightarrow \mathbb{R}$.&lt;/br&gt;
    We denote the $n$-th value of the function as $f[n]$.
    &lt;small&gt;
      The definition provided is very similar to the one of a sequence. We will use this parallelism to derive some theorems and properties of signals. In particular, we will use $\ell^{p}(\mathbb{R})$ spaces.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
      A very particular signal is the discrete Dirac delta.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    The discrete &lt;strong&gt;Dirac delta&lt;/strong&gt; is the signal
    $\delta[n] =
    \begin{cases}
    1 &amp; \text{if } n = 0 \\
    0 &amp; \text{if } n \neq 0
    \end{cases}$.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;discrete-convolution&quot;&gt;Discrete convolution&lt;/h1&gt;
  &lt;p&gt;
    The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real signals, the &lt;strong&gt;discrete convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}.
    $$
  &lt;/p&gt;
  &lt;p&gt;
    The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$.
    When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$.
    &lt;small&gt;
      The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: &lt;/br&gt;
      $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    In general, the summation in the convolution definition is not always finite.
    Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals
    with $p, q \in [1, \infty]$ such that
    $$
    \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\},
    $$
    then $f * g$ is bounded.
    &lt;small&gt;
      For every $p &gt; 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences,
      i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if
      $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p}&lt; +\infty$.&lt;/br&gt;
      $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that
    $$
    |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}.
    $$
    Using Hölder's inequality and with a simple change of variable, we prove the theorem:
    &lt;small&gt;
    Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$
    if $p$ and $q$ satisfy the condition of the theorem.
    The special case $p=q=2$ gives the Cauchy–Schwarz inequality.
    &lt;/small&gt;
    $$
    \begin{aligned}
        |(f*g)[n]| &amp; \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\
        &amp; \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} &amp; \textit{\footnotesize Hölder's ineq.} \\
        &amp; = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} &amp; {\footnotesize k' = n - k } \\
        &amp; = ||f||_{p}||g||_{q}. &amp; \square
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p&gt;
    Two remarkable properties of the convolution are commutativity and linearity.
    These two properties are sufficient to explain why convolutions are a fundamental component
    of many processing systems.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g$ be two real signals such that its convolution is well-defined, then
    $$
    (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    That means that the convolution is a &lt;strong&gt;commutative&lt;/strong&gt; operation.
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$.
    $$
    \begin{aligned}
    (f * g)[n] &amp; = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\
    &amp; = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n]. &amp; \hspace*{6em} \square
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below
    are all well-defined, then we have
    $$
    ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    Thus the convolution is a &lt;strong&gt;linear&lt;/strong&gt; operation.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators&quot;&gt;Operators&lt;/h1&gt;
    &lt;p&gt;
        To extract information from a stream of data, we have to process it.
        Every operation applied to a signal is associated with an operator.
        For example, we can use operators to reduce noise, to recognize features,
        to detect peaks or discontinuities.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is &lt;strong&gt;linear&lt;/strong&gt; if $\forall\: a, b \in \mathbb{R}$
      $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator is &lt;strong&gt;shift-invariant&lt;/strong&gt; if $\forall\: p \in \mathbb{Z}$
      $$L(f[n-p]) = L(f)[n-p].$$
    &lt;/p&gt;
    &lt;p&gt;
      When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant.
      The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters.
    &lt;/p&gt;
    &lt;p&gt;
      This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators-convolution&quot;&gt;Operators and convolution&lt;/h1&gt;
    &lt;p&gt;
      There is a tight relationship between linear shift-invariant operators and convolution.
      To reveal this connection, we need to recall the discrete Dirac.
      Translating the discrete Dirac by $p$, we obtain
      $$
      \delta[n - p] =
      \begin{cases}
      1 &amp; n = p \\
      0 &amp; n \neq p
      \end{cases}.
      $$
      Using this expression, we can represent every signal as a linear combination of Dirac signals
      $$
      f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
      $$
      Now we have all the elements necessary to prove the main theorem of the article.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    &lt;/p&gt;
    &lt;h3&gt;Proof&lt;/h3&gt;
    &lt;p&gt;
        If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$.
        Setting $h[n] = L(\delta)[n]$, we have
        $$
        \begin{aligned}
        L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp;
        \end{aligned}
        $$
        To prove the inverse implication, we have to show that for every signal $h[n]$,
        $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution.
        Now, we show that the resulting operation is also shift-invariant.
        $$
        \begin{aligned}
            L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\
            &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\
            &amp; = L(f)[n - p]. &amp; \square
        \end{aligned}
        $$
    &lt;/p&gt;
    &lt;p&gt;
      This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations.
      We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons.
      Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines.
    &lt;/p&gt;
    &lt;p&gt;
      Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;generalizations&quot;&gt;Generalizations&lt;/h1&gt;
  &lt;p&gt;
    In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real function, the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt  \quad \forall\: x \in \mathbb{R}.
    $$
  &lt;/p&gt;
  &lt;p&gt;
    All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions.
    &lt;small&gt;
      Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension.
    We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals,
    the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k]  \quad \forall\: n \in \mathbb{Z}^{d}.
    $$
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;cnn&quot;&gt;Convolutional Neural Networks&lt;/h1&gt;
    &lt;p&gt;
      The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision.
      A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling.
    &lt;/p&gt;
    &lt;p&gt;
      Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant.
    &lt;/p&gt;
    &lt;p&gt;
      Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen.
    &lt;/p&gt;
    &lt;p&gt;
      Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation!
      A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper &quot;&lt;a href=https://arxiv.org/abs/1904.11486&gt;Making Convolutional Networks Shift-Invariant Again&lt;/a&gt;&quot;.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
    &lt;p&gt;
      The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications.
    &lt;/p&gt;
&lt;/section&gt;
&lt;footer style=&quot;text-align: left&quot;&gt;
  &lt;section&gt;
    &lt;h1&gt;References&lt;/h1&gt;
    &lt;ol&gt;
      &lt;li id=&quot;ref-1&quot;&gt;
        Mallat, Stéphane. &lt;em&gt;A wavelet tour of signal processing&lt;/em&gt;. Elsevier, 1999.
      &lt;/li&gt;
      &lt;li id=&quot;ref-2&quot;&gt;
        Zhang, Richard. &lt;em&gt;Making convolutional networks shift-invariant again&lt;/em&gt;. arXiv preprint arXiv:1904.11486 (2019).
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/section&gt;
  &lt;section&gt;
    &lt;h1&gt;Updates and Corrections&lt;/h1&gt;
    &lt;p&gt;If you see mistakes or want to suggest changes, please &lt;a href=&quot;https://github.com/lucagrementieri/lucagrementieri.github.io&quot;&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt;
  &lt;/section&gt;
&lt;/footer&gt;</content><author><name>Luca Grementieri</name></author><summary type="html">Signals There are many kinds of data: images, sounds, text, time series are very well-known examples. All these data are discrete. In some cases, this property is intrinsic: for example, written text is naturally a sequence of characters and words. In most cases, when data are the result of a measurement, analog signals are digitized during the acquisition process. The devices that perform the measurements are called sensors. Sensors produce discrete signals by sampling or aggregating information from the analog world around us. For example, a sound recorder samples audio waves to register a discretized version of them. On the other hand, a camera collects photons in every pixel aggregating the light coming from all directions in a short amount of time. From a mathematical point of view, a signal is any function defined on a discrete set. This definition is very general, and it includes all the examples above. DEFINITION A discrete signal is a function whose domain is $\mathbb{Z}$, $f: \mathbb{Z} \longrightarrow \mathbb{R}$. We denote the $n$-th value of the function as $f[n]$. The definition provided is very similar to the one of a sequence. We will use this parallelism to derive some theorems and properties of signals. In particular, we will use $\ell^{p}(\mathbb{R})$ spaces. A very particular signal is the discrete Dirac delta. DEFINITION The discrete Dirac delta is the signal $\delta[n] = \begin{cases} 1 &amp; \text{if } n = 0 \\ 0 &amp; \text{if } n \neq 0 \end{cases}$. Discrete convolution The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions. DEFINITION Let $f$, $g$ be two real signals, the discrete convolution of $f$ and $g$ is $$ (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}. $$ The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$. When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$. The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$. In general, the summation in the convolution definition is not always finite. Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element. THEOREM Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals with $p, q \in [1, \infty]$ such that $$ \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\}, $$ then $f * g$ is bounded. For every $p &gt; 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences, i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p} $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences. Proof $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that $$ |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}. $$ Using Hölder's inequality and with a simple change of variable, we prove the theorem: Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$ if $p$ and $q$ satisfy the condition of the theorem. The special case $p=q=2$ gives the Cauchy–Schwarz inequality. $$ \begin{aligned} |(f*g)[n]| &amp; \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\ &amp; \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}} \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} &amp; \textit{\footnotesize Hölder's ineq.} \\ &amp; = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}} \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} &amp; {\footnotesize k' = n - k } \\ &amp; = ||f||_{p}||g||_{q}. &amp; \square \end{aligned} $$ Two remarkable properties of the convolution are commutativity and linearity. These two properties are sufficient to explain why convolutions are a fundamental component of many processing systems. THEOREM Let $f, g$ be two real signals such that its convolution is well-defined, then $$ (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}. $$ That means that the convolution is a commutative operation. Proof For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$. $$ \begin{aligned} (f * g)[n] &amp; = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\ &amp; = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n]. &amp; \hspace*{6em} \square \end{aligned} $$ THEOREM Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below are all well-defined, then we have $$ ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}. $$ Thus the convolution is a linear operation. Operators To extract information from a stream of data, we have to process it. Every operation applied to a signal is associated with an operator. For example, we can use operators to reduce noise, to recognize features, to detect peaks or discontinuities. DEFINITION A discrete operator $L$ is linear if $\forall\: a, b \in \mathbb{R}$ $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$ DEFINITION A discrete operator is shift-invariant if $\forall\: p \in \mathbb{Z}$ $$L(f[n-p]) = L(f)[n-p].$$ When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant. The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters. This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series. Operators and convolution There is a tight relationship between linear shift-invariant operators and convolution. To reveal this connection, we need to recall the discrete Dirac. Translating the discrete Dirac by $p$, we obtain $$ \delta[n - p] = \begin{cases} 1 &amp; n = p \\ 0 &amp; n \neq p \end{cases}. $$ Using this expression, we can represent every signal as a linear combination of Dirac signals $$ f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]. $$ Now we have all the elements necessary to prove the main theorem of the article. THEOREM A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete signal $h[n]$ such that for every signal $f[n]$ $$ L(f)[n] = (f * h)[n]. $$ Proof If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$. Setting $h[n] = L(\delta)[n]$, we have $$ \begin{aligned} L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp; \end{aligned} $$ To prove the inverse implication, we have to show that for every signal $h[n]$, $L(f) = f * h$ is a linear shift-invariant operator. The linearity comes from the linearity of the convolution. Now, we show that the resulting operation is also shift-invariant. $$ \begin{aligned} L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\ &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\ &amp; = L(f)[n - p]. &amp; \square \end{aligned} $$ This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations. We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons. Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines. Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too. Generalizations In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented. DEFINITION Let $f$, $g$ be two real function, the convolution of $f$ and $g$ is $$ (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt \quad \forall\: x \in \mathbb{R}. $$ All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions. Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can. Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension. We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case. DEFINITION Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals, the convolution of $f$ and $g$ is $$ (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}^{d}. $$ Convolutional Neural Networks The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision. A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling. Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant. Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen. Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation! A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper &quot;Making Convolutional Networks Shift-Invariant Again&quot;. Conclusions The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications. References Mallat, Stéphane. A wavelet tour of signal processing. Elsevier, 1999. Zhang, Richard. Making convolutional networks shift-invariant again. arXiv preprint arXiv:1904.11486 (2019). Updates and Corrections If you see mistakes or want to suggest changes, please open an issue on GitHub.</summary></entry></feed>