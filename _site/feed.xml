<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-02T17:54:10+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Luca Grementieri</title><subtitle>Scientific blog about maths and machine learning</subtitle><author><name>Luca Grementieri</name></author><entry><title type="html">ReLU networks are universal approximators</title><link href="http://localhost:4000/2020-05-02/relu-universal-approximators" rel="alternate" type="text/html" title="ReLU networks are universal approximators" /><published>2020-05-02T00:00:00+02:00</published><updated>2020-05-02T00:00:00+02:00</updated><id>http://localhost:4000/2020-05-02/relu-universal-approximators</id><content type="html" xml:base="http://localhost:4000/2020-05-02/relu-universal-approximators">$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}

\gdef\R{\mathbb{R}}
\gdef\Rn{\mathbb{R}^{n}}
\gdef\M{\mathcal{M}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
\gdef\max#1{\underset{#1}{\text{max}}}
$
&lt;section&gt;
  &lt;h1 id=&quot;universal-approximation-theorem&quot;&gt;Universal approximation theorem&lt;/h1&gt;
  &lt;p&gt;
    The most known theoretic result about neural networks is that they are universal approximators.
    This statement is quite vague since it does not specify which types of neural networks models are considered and the precise distance used to measure the quality of the approximation.
    We are going to clarify those terms and prove the theorem in its most general form.
  &lt;/p&gt;
  &lt;p&gt;
    The considered neural network model is a multilayer perceptron (MLP) with a single hidden layer,
    $n$-dimensional input $\x$ and real output $y$.
    The details of such a model can be found in a &lt;a href=&quot;../2020-04-26/ridge-functions.html&quot;&gt;previous article&lt;/a&gt;.
    The mathematical form of functions expressed by this model is
    $$
    y = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right),
    $$
    where $\x \in \Rn$ is the input vectors and $\w^i \in \Rn$, $\theta_i, c_i \in \R$ are parameters and $\sigma$ is a univariate continuous function said &lt;em&gt;activation function&lt;/em&gt;.
  &lt;/p&gt;
  &lt;p&gt;
    The parameter $r$ represents the number of units in the hidden layer. It controls the number of functions $\sigma(\w^i \cdot \x)$ that are linearly combined to obtain $y$.
    If we consider the class of functions described by all MLPs, with any number of hidden units, we obtain
    the linear vector space of functions
    $$
    \M(\sigma) = \text{span}\{\sigma(\w \cdot \x - \theta), \theta \in \R, \w \in \Rn\}.
    $$
    The universal approximation theorem establishes the approximation properties of $\M(\sigma)$.
  &lt;/p&gt;
  &lt;p&gt;
    Now that the neural network model is specified, the only missing piece is a sound definition of a universal approximator.
    The definition involves many topological concepts (like compactness, uniform convergence and density) that we have already introduced in a &lt;a href=&quot;../2020-04-26/ridge-functions.html&quot;&gt;previous article&lt;/a&gt;, so we take them for granted.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A class of functions is a &lt;strong&gt;universal approximator&lt;/strong&gt; if it is dense in $C(\Rn)$
    in the topology of uniform convergence on compact subsets.
  &lt;/p&gt;
  &lt;p&gt;
    We have all the ingredients to state the universal approximation theorem.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    The class of multilayer perceptrons with a single hidden layer and continuous activation function $\sigma$ is
    a universal approximator if and only if $\sigma$ is not a polynomial.
  &lt;/p&gt;
  &lt;p&gt;
    This form of the theorem is the most general one and it is due to Pinkus et al.
    Many weaker forms are known, in particular Cybenko first proved the property for sigmoidal activation functions.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;mlp&quot;&gt;Multilayer perceptron&lt;/h1&gt;
  &lt;p&gt;
    Neural networks encode a subset of continuous functions. To examine the approximation capabilities of this class, we introduce the multiplayer perceptron and one of its generalization.
  &lt;/p&gt;
  &lt;p&gt;
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of &lt;em&gt;layers&lt;/em&gt;.
    Every layer is composed by &lt;em&gt;neurons&lt;/em&gt;, the elementary processing units of the network.
    Every artificial neuron in the intermediate &lt;em&gt;hidden layers&lt;/em&gt; processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the &lt;em&gt;activation function&lt;/em&gt; $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    &lt;small&gt;
      In scientific literature about neural networks, the values $w_{i}$ are called &lt;em&gt;weights&lt;/em&gt;. They model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron.
      $\theta$, called &lt;em&gt;bias&lt;/em&gt;, recalls the threshold potential involved in the firing of biologic neurons.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    In the final layer, said the &lt;em&gt;output layer&lt;/em&gt;, neurons operate differently. They perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this shallow model because it can already approximate a large class of functions.
    &lt;small&gt;
      The considered MLP model outputs a single real number.
      The analysis can be generalized to outputs in $\R^{m}$ by repeating it on every component.
      A model whose output is multi-dimensional can be used for both regression and classification tasks. Indeed, classification problems are framed as a regression of class probabilities.
    &lt;/small&gt;
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;ridge&quot;&gt;Ridge functions&lt;/h1&gt;
  &lt;p&gt;
    The function $N(\x)$ has a particular structure: it is the composition of a univariate function sigma with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A &lt;strong&gt;ridge function&lt;/strong&gt; is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  &lt;/p&gt;
  &lt;p&gt;
    The vector $\a$ is called &lt;em&gt;direction&lt;/em&gt; because a ridge function is constant on the parallel hyperplanes orthogonal to $\a$. These hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  &lt;/p&gt;
  &lt;p&gt;
    We denote the set of ridge functions with direction $\a$
    $$\overline{\RR}(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\overline{\RR}(\a) = \overline{\RR}(\bold{b})$.
  &lt;/p&gt;
  &lt;p&gt;
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \overline{\RR}(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A &lt;em&gt;span&lt;/em&gt; of a set $S$ is the set of linear combinations of elements in $S$, then
    every $F \in \overline{\RR}(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\overline{\RR}(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
  &lt;/p&gt;
  &lt;p&gt;
    If we require the continuity of the activation function $\sigma$ in a MLP, the functions defined by a MLP
    belong to the smaller set of linear combinations of continuous ridge functions
    $$
    \RR(\Omega) = \text{span}\{f(\a \cdot \x), f \in C(\R), \a \in \Omega\}
    $$
    with $\Omega = \Rn$.
  &lt;/p&gt;
  &lt;p&gt;
    We are interested in the approximation theory of the functional vector spaces $\RR(\Omega)$ to characterize the class of functions defined by MLPs.
    If a MLP is capable of approximating a function, the same holds for $\RR(\Rn)$, a larger family of functions.
    Conversely, if $\RR(\Rn)$ cannot approximate a function, then neither MLPs could.
    Therefore $\RR(\Rn)$ gives us an upper bound on the approximation capabilities of MLPs.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;compact-convergence&quot;&gt;Uniform convergence on compact sets&lt;/h1&gt;
  &lt;p&gt;
    The breadth of the class of continuous functions $\RR(\Omega)$ depends on $\Omega$.
    For instance, we have already seen that, $\RR(\{\a\})$ represents a limited family of functions constant on parallel hyperplanes.
    Thus, not every $\RR(\Omega)$ is a dense subspace of $C(\Rn)$.
    We will present the conditions on $\Omega$ that ensure that this property holds.
  &lt;/p&gt;
  &lt;p&gt;
    Since density is a property of a topological space, we have to define a topology
    on $C(\Rn)$ to clarify the meaning of this concept.
    In the approximation theory of continuous functions, the topology of uniform
    convergence on compact subsets is the most common.
    &lt;small&gt;
      The general topological definition of a compact involves covers and subcovers.
      The Heine-Borel theorem gives a much simpler characterization of compactness in a euclidean space: in $\Rn$,
      a set is compact if and only if it is closed and bounded.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A sequence of functions $f_{m} \in C(\Rn), m \in \mathbb{N}$ is said to
    &lt;strong&gt;converge uniformly on compact sets&lt;/strong&gt; as $m \to \infty$ to some function
    $f \in C(\Rn)$ if, for every compact set $K \subset \Rn$ and every $\epsilon &gt; 0$,
    there exist $\overline{m}$ such that $\forall\, m \geq \overline{m}$
    $$
    ||f_{m} - f||_{K} = \max{x \in K}\ | f_{m}(x) - f(x)| &lt; \epsilon.
    $$
    &lt;small&gt;
      If we can prove density in this topology, then the subset is also dense in many other topologies.
      For example, density would hold in $L^{p}(K)$, where $K$ is any compact subset of $\Rn$ and $p \in [1,\infty[$.
      Indeed $||f||_{p,K} \leq ||f||_{K} &lt; +\infty$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    A very powerful tool to prove results with this topology is the &lt;strong&gt;Stone-Weierstrass Theorem&lt;/strong&gt;.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Suppose $X$ is a compact Hausdorff space and $C(X)$ is the space of continuous real-valued functions over $X$.
    A subalgebra $A$ of $C(X)$ containing a non-zero constant function, is dense in $C(X)$ if and only if it separates points.
    &lt;small&gt;
      A set of real-valued functions $S$ separates points if for any pair $x, y \in \R$, there exists a function
      $f \in S$ such that $f(x) \neq f(y)$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    This theorem implies Weierstrass Approximation Theorem: indeed,  polynomials on $[a, b]$ form a subalgebra of $C([a, b])$, which contains constant functions and separates points.
    Let us recall the definition of algebra and subalgebra.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $A$ be a vector space over $\R$ equipped with a binary operation (a product) from $\cdot: A \times A \to A$. Then $A$ is an &lt;strong&gt;algebra&lt;/strong&gt; if the following identities hold for all elements $x$, $y$, and $z$ of $A$, and all scalars $a$ and $b$ of $\R$:
    $$
    \begin{aligned}
    (x + y) \cdot z &amp;= x \cdot z + y \cdot z; &amp; \qquad \textit{\footnotesize right distributivity}\\
    z \cdot (x + y) &amp;= z \cdot x + z \cdot y; &amp; \qquad \textit{\footnotesize left distributivity}\\
    (ax) \cdot (by) &amp;= (ab) (x \cdot y). &amp; \qquad \textit{\footnotesize compatibility with scalars}
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A &lt;strong&gt;subalgebra&lt;/strong&gt; is a subset of an algebra, closed under addition, multiplication by scalar and
    product.
    &lt;small&gt;
      A set is closed under an operation if the result of the operation between elements of the set is still included in the set itself.
    &lt;/small&gt;
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;density&quot;&gt;Density of continuous ridge functions&lt;/h1&gt;
  &lt;p&gt;
    We now have all the ingredients to state a density theorem about $\RR(\Rn)$ in $C(\Rn)$.
    Actually, the theorem is even stronger because the proof shows that $\RR(\mathbb{Z}^{n})$ is dense in $C(\Rn)$.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    $\RR(\Rn)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets.
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    It is sufficient to prove that linear combinations of the functions $e^{\bold{n} \cdot \x}$,
    where $\bold{n} \in \mathbb{Z}^{n}$, are dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets. This fact is a consequence of the Stone-Weierstrass Theorem.
  &lt;/p&gt;
  &lt;p&gt;
    For every compact set $K \subset \Rn$, $C(K)$ is an algebra: it is a vector space, and the product between functions satisfies all the properties listed in the definition of algebra.
    The set $\mathcal{E}_{K} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}, x \in K \}$ is a linear subspace of $C(K)$, therefore it is closed under addition and multiplication by a scalar.
    Thanks to the properties of the exponential function, the product of two elements $f, g, \in \mathcal{E}_{K}$ is still included in $\mathcal{E}_{K}$:
    $$
    f = \sum_{i=1}^s e^{\bold{n}_i \cdot \x} \quad g = \sum_{j=1}^t e^{\bold{m}_j \cdot \x} \Rightarrow f \cdot g = \sum_{i=1}^s \sum_{j=1}^t e^{(\bold{n}_i + \bold{m}_j) \cdot x}.
    $$
    Thus, $\mathcal{E}_{K}$ is a subalgebra of $C(K)$.
  &lt;/p&gt;
  &lt;p&gt;
    Putting $\bold{n} = \bold{0}$, we obtain
    the non-zero constant function $e^{\bold{0} \cdot \x} = 1$.
    Moreover $\mathcal{E}_{K}$ separates points. Let $\bold{y} \neq \bold{z} \in \Rn$, then
    there exists a coordinate $h \in \{1, \mathellipsis, n\}$ such that $y_h \neq z_h$.
    The function $e^{\bold{e}_h \cdot \x}$, where $\bold{e}_h = (0, \mathellipsis, 0, 1, 0, \mathellipsis, 0)$
    is $h$-th vector of the canonical basis, separates $\bold{y}$ from $\bold{z}$
    $$e^{\bold{e}_h \cdot y} = e^{y_h} \neq e^{z_h} = e^{\bold{e}_h \cdot z}.$$
  &lt;/p&gt;
  &lt;p&gt;
    All the hypotheses of Stone-Weierstrass theorem are satisfied. Thus, for every compact $K$, $\mathcal{E}_{K}$
    is dense in $C(K)$ with respect to the uniform norm. That is equivalent to say that $\mathcal{E} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}\}$ is dense in $C(\Rn)$ in the topology of
    uniform convergence on compact subsets.
    &lt;span style=&quot;float:right&quot;&gt;$\square$&lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
    The density of $\RR(\Rn)$ in $C(\Rn)$ tells us that for every continuous functions $f$ it exists a sequence
    of functions in $\RR(\Rn)$ whose limit is $f$ in the chosen topology.
    Thus, any continuous function can be approximated by functions in $\RR(\Rn)$ with an arbitrary small
    approximation error on every compact of $\Rn$.
    &lt;small&gt;
      The uniform convergence on compact sets is not equivalent to uniform convergence.
      In practice, that difference is irrelevant as we always approximate functions whose domain can be included in a compact set. Most of the time, the approximated function is known only on a finite set of points.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    We have already observed that every $\RR(\Omega)$ only depends on the set of normalized directions in $\Omega$. By consequence, the theorem implies that $\RR([-M, M]^n)$ is dense in $C(\Rn)$ for an arbitrary constant $M$.
    In general, we can bound the norm of ridge function directions without any loss in expressive power.
  &lt;/p&gt;
  &lt;p&gt;
    The theorem implies nothing about MLPs. A negative result would have been valid for any subset of $\RR(\Rn)$, while such a positive result only give us an idea of what kind of approximation the MLPs could attain and under which conditions.
  &lt;/p&gt;
  &lt;p&gt;
    A more general theorem by Vostrecov and Kreines states a necessary and sufficient condition
    for the density of $\RR(\Omega)$ in $C(\Rn)$.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    $\RR(\Omega)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets,
    if and only if there is no homogeneous polynomial $p \neq 0$ such that $p(\x) = 0 \ \forall\, \x \in \Omega$.
  &lt;/p&gt;
  &lt;p&gt;
    This final theorem provides the complete characterization of the density of linear combination of ridge functions. We have not discussed if all these properties are preserved by MLPs, but this is a solid starting point to deepen the subject.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
    &lt;p&gt;
      Single hidden layer MLPs are a particular linear combination of ridge functions.
      We have proved that the class of functions spanned by continuous ridge functions approximates every continuous function with arbitrary precision.
      This fact is relevant in applications because it allows designing parametric models without imposing compelling constraints.
    &lt;/p&gt;
    &lt;p&gt;
      The density of $\RR(\Omega)$ in $C(\Rn)$ does not imply that the same property holds for MLPs, but we can hope that functions represented by MLPs have the same approximation capability.
      Actually, like the linear combinations of ridge functions, MLPs are universal approximators.
      That will be the topic of a follow-up article.
    &lt;/p&gt;
&lt;/section&gt;
&lt;footer style=&quot;text-align: left&quot;&gt;
  &lt;section&gt;
    &lt;h1&gt;References&lt;/h1&gt;
    &lt;ol&gt;
      &lt;li id=&quot;ref-1&quot;&gt;
        Pinkus, Allan. &lt;em&gt;Ridge functions.&lt;/em&gt; Vol. 205. Cambridge University Press, 2015.
      &lt;/li&gt;
      &lt;li id=&quot;ref-2&quot;&gt;
        Pinkus, Allan. &lt;em&gt;Approximation theory of the MLP model in neural networks.&lt;/em&gt; Acta numerica 8 (1999): 143-195.
      &lt;/li&gt;
      &lt;li id=&quot;ref-3&quot;&gt;
        Vostrecov, B. A., and M. A. Kreines. &lt;em&gt;Approximation of continuous functions by superpositions of plane waves.&lt;/em&gt; Dokl. Akad. Nauk SSSR. Vol. 140. 1961.
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/section&gt;
  &lt;section&gt;
    &lt;h1&gt;Updates and Corrections&lt;/h1&gt;
    &lt;p&gt;If you see mistakes or want to suggest changes, please &lt;a href=&quot;https://github.com/lucagrementieri/lucagrementieri.github.io&quot;&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt;
  &lt;/section&gt;
&lt;/footer&gt;</content><author><name>Luca Grementieri</name></author><summary type="html">$ \gdef\x{\bold{x}} \gdef\w{\bold{w}}</summary></entry><entry><title type="html">Artificial neurons as ridge functions</title><link href="http://localhost:4000/2020-04-26/ridge-functions" rel="alternate" type="text/html" title="Artificial neurons as ridge functions" /><published>2020-04-26T00:00:00+02:00</published><updated>2020-04-26T00:00:00+02:00</updated><id>http://localhost:4000/2020-04-26/ridge-functions</id><content type="html" xml:base="http://localhost:4000/2020-04-26/ridge-functions">$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}
\gdef\a{\bold{a}}

\gdef\R{\mathbb{R}}
\gdef\RR{\mathcal{R}}
\gdef\Rn{\mathbb{R}^{n}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
\gdef\max#1{\underset{#1}{\text{max}}}
$
&lt;section&gt;
  &lt;h1 id=&quot;approximation-theory&quot;&gt;Approximation theory&lt;/h1&gt;
  &lt;p&gt;
    Approximation theory is the branch of mathematics that studies how to approximate a large class of functions combining simpler ones.
    It looks for sets of functions described by a finite or countable number of parameters that can faithfully reproduce functional classes with an uncountable number of dimensions. Continuous mappings are the most studied example of such a large family.
  &lt;/p&gt;
  &lt;p&gt;
    This topic has a great practical interest in solving optimization problems on functions without making strong hypotheses on the optimal solution.
    Given a functional $\ell: \mathcal{F} \longrightarrow \R$,
    an optimization problem takes the form
    $$
    f^{*} = \argmin{f \in \mathcal{F}}\ \ell(f),
    $$
    where $\mathcal{F}$ is the search space of functions determined by the assumptions on $f^{*}$.
    For example, if we know that the solution $f^{*}$ is a continuous univariate function, then $\mathcal{F} = C(\R)$.
  &lt;/p&gt;
  &lt;p&gt;
    In practice, we cannot solve the optimization problem on a function space with an infinite number of dimensions, because machines can work only with a finite number of parameters. So we need to restrict $\mathcal{F}$ to a more
    manageable subset $\mathcal{G} \subset \mathcal{F}$.
    This restriction adds more assumptions on $f^{*}$. Computational requirements dictates these assumptions, not the problem itself. For instance, in many cases, the search space is restricted to linear functions because of their simplicity and interpretability.
  &lt;/p&gt;
  &lt;p&gt;
    Representing uncountable dimensions with a countable amount of them seems impossible. The density property makes it possible. We use this same property in every calculation on a computer.
    Since $\mathbb{Q}$ is dense in $\R$, we can represent real numbers on machines with a precision that is only limited by the available memory.
  &lt;/p&gt;
  &lt;p&gt;
    Similarly, the approximation theory of functions looks for a countable set $\mathcal{G}$ dense in $\mathcal{F}$.
    Density assures that the actual search space is not reduced because functions in $\mathcal{G}$ can reproduce $f^{*}$ with arbitrary precision. Let us recall the definition of a dense set.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $X$ be a topological space and let $A \subseteq X$. $A$ is &lt;strong&gt;dense&lt;/strong&gt; in $X$
    if for every $x \in X$, any neighborhood of $x$ contains at least one point from $A$.
    Equivalently, $A$ is dense in $X$ if the closure of $A$ is $X$: $\overline{A} = X$.
    &lt;small&gt;
      The definitions of neighborhood and closure come from general topology. A neighborhood of a point $x$ is a set that includes an open subset containing $x$. The closure of a set $S$ is the smallest closed set that covers $S$. It is the union of $S$ and its limit points.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    The most known result of approximation theory is &lt;strong&gt;Weierstrass Approximation Theorem&lt;/strong&gt;.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f$ be a continuous real-valued function defined on the interval $[a, b]$.
    For every $\epsilon &gt; 0$, there exists a polynomial $p$ such that $\forall\, x \in [a, b]$, we have
    $|f(x) − p(x)| &lt; \epsilon$, or equivalently, $||f−p||_{\infty} &lt; \epsilon$.
    &lt;small&gt;
      The notation $||\cdot||_{\infty}$ indicates the uniform or supremum norm.
      The norm assigns to a bounded function $f$ on the interval $[a, b]$ the value
      $||f||_{\infty} = \text{sup}\{|f(x)|, x \in [a, b]\}$.
    &lt;/small&gt;
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;mlp&quot;&gt;Multilayer perceptron&lt;/h1&gt;
  &lt;p&gt;
    Neural networks encode a subset of continuous functions. To examine the approximation capabilities of this class, we introduce the multiplayer perceptron and one of its generalization.
  &lt;/p&gt;
  &lt;p&gt;
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of &lt;em&gt;layers&lt;/em&gt;.
    Every layer is composed by &lt;em&gt;neurons&lt;/em&gt;, the elementary processing units of the network.
    Every artificial neuron in the intermediate &lt;em&gt;hidden layers&lt;/em&gt; processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the &lt;em&gt;activation function&lt;/em&gt; $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    &lt;small&gt;
      In scientific literature about neural networks, the values $w_{i}$ are called &lt;em&gt;weights&lt;/em&gt;. They model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron.
      $\theta$, called &lt;em&gt;bias&lt;/em&gt;, recalls the threshold potential involved in the firing of biologic neurons.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    In the final layer, said the &lt;em&gt;output layer&lt;/em&gt;, neurons operate differently. They perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this shallow model because it can already approximate a large class of functions.
    &lt;small&gt;
      The considered MLP model outputs a single real number.
      The analysis can be generalized to outputs in $\R^{m}$ by repeating it on every component.
      A model whose output is multi-dimensional can be used for both regression and classification tasks. Indeed, classification problems are framed as a regression of class probabilities.
    &lt;/small&gt;
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;ridge&quot;&gt;Ridge functions&lt;/h1&gt;
  &lt;p&gt;
    The function $N(\x)$ has a particular structure: it is the composition of a univariate function sigma with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A &lt;strong&gt;ridge function&lt;/strong&gt; is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  &lt;/p&gt;
  &lt;p&gt;
    The vector $\a$ is called &lt;em&gt;direction&lt;/em&gt; because a ridge function is constant on the parallel hyperplanes orthogonal to $\a$. These hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  &lt;/p&gt;
  &lt;p&gt;
    We denote the set of ridge functions with direction $\a$
    $$\overline{\RR}(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\overline{\RR}(\a) = \overline{\RR}(\bold{b})$.
  &lt;/p&gt;
  &lt;p&gt;
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \overline{\RR}(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A &lt;em&gt;span&lt;/em&gt; of a set $S$ is the set of linear combinations of elements in $S$, then
    every $F \in \overline{\RR}(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\overline{\RR}(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
  &lt;/p&gt;
  &lt;p&gt;
    If we require the continuity of the activation function $\sigma$ in a MLP, the functions defined by a MLP
    belong to the smaller set of linear combinations of continuous ridge functions
    $$
    \RR(\Omega) = \text{span}\{f(\a \cdot \x), f \in C(\R), \a \in \Omega\}
    $$
    with $\Omega = \Rn$.
  &lt;/p&gt;
  &lt;p&gt;
    We are interested in the approximation theory of the functional vector spaces $\RR(\Omega)$ to characterize the class of functions defined by MLPs.
    If a MLP is capable of approximating a function, the same holds for $\RR(\Rn)$, a larger family of functions.
    Conversely, if $\RR(\Rn)$ cannot approximate a function, then neither MLPs could.
    Therefore $\RR(\Rn)$ gives us an upper bound on the approximation capabilities of MLPs.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;compact-convergence&quot;&gt;Uniform convergence on compact sets&lt;/h1&gt;
  &lt;p&gt;
    The breadth of the class of continuous functions $\RR(\Omega)$ depends on $\Omega$.
    For instance, we have already seen that, $\RR(\{\a\})$ represents a limited family of functions constant on parallel hyperplanes.
    Thus, not every $\RR(\Omega)$ is a dense subspace of $C(\Rn)$.
    We will present the conditions on $\Omega$ that ensure that this property holds.
  &lt;/p&gt;
  &lt;p&gt;
    Since density is a property of a topological space, we have to define a topology
    on $C(\Rn)$ to clarify the meaning of this concept.
    In the approximation theory of continuous functions, the topology of uniform
    convergence on compact subsets is the most common.
    &lt;small&gt;
      The general topological definition of a compact involves covers and subcovers.
      The Heine-Borel theorem gives a much simpler characterization of compactness in a euclidean space: in $\Rn$,
      a set is compact if and only if it is closed and bounded.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A sequence of functions $f_{m} \in C(\Rn), m \in \mathbb{N}$ is said to
    &lt;strong&gt;converge uniformly on compact sets&lt;/strong&gt; as $m \to \infty$ to some function
    $f \in C(\Rn)$ if, for every compact set $K \subset \Rn$ and every $\epsilon &gt; 0$,
    there exist $\overline{m}$ such that $\forall\, m \geq \overline{m}$
    $$
    ||f_{m} - f||_{K} = \max{x \in K}\ | f_{m}(x) - f(x)| &lt; \epsilon.
    $$
    &lt;small&gt;
      If we can prove density in this topology, then the subset is also dense in many other topologies.
      For example, density would hold in $L^{p}(K)$, where $K$ is any compact subset of $\Rn$ and $p \in [1,\infty[$.
      Indeed $||f||_{p,K} \leq ||f||_{K} &lt; +\infty$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    A very powerful tool to prove results with this topology is the &lt;strong&gt;Stone-Weierstrass Theorem&lt;/strong&gt;.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Suppose $X$ is a compact Hausdorff space and $C(X)$ is the space of continuous real-valued functions over $X$.
    A subalgebra $A$ of $C(X)$ containing a non-zero constant function, is dense in $C(X)$ if and only if it separates points.
    &lt;small&gt;
      A set of real-valued functions $S$ separates points if for any pair $x, y \in \R$, there exists a function
      $f \in S$ such that $f(x) \neq f(y)$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    This theorem implies Weierstrass Approximation Theorem: indeed,  polynomials on $[a, b]$ form a subalgebra of $C([a, b])$, which contains constant functions and separates points.
    Let us recall the definition of algebra and subalgebra.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $A$ be a vector space over $\R$ equipped with a binary operation (a product) from $\cdot: A \times A \to A$. Then $A$ is an &lt;strong&gt;algebra&lt;/strong&gt; if the following identities hold for all elements $x$, $y$, and $z$ of $A$, and all scalars $a$ and $b$ of $\R$:
    $$
    \begin{aligned}
    (x + y) \cdot z &amp;= x \cdot z + y \cdot z; &amp; \qquad \textit{\footnotesize right distributivity}\\
    z \cdot (x + y) &amp;= z \cdot x + z \cdot y; &amp; \qquad \textit{\footnotesize left distributivity}\\
    (ax) \cdot (by) &amp;= (ab) (x \cdot y). &amp; \qquad \textit{\footnotesize compatibility with scalars}
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A &lt;strong&gt;subalgebra&lt;/strong&gt; is a subset of an algebra, closed under addition, multiplication by scalar and
    product.
    &lt;small&gt;
      A set is closed under an operation if the result of the operation between elements of the set is still included in the set itself.
    &lt;/small&gt;
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;density&quot;&gt;Density of continuous ridge functions&lt;/h1&gt;
  &lt;p&gt;
    We now have all the ingredients to state a density theorem about $\RR(\Rn)$ in $C(\Rn)$.
    Actually, the theorem is even stronger because the proof shows that $\RR(\mathbb{Z}^{n})$ is dense in $C(\Rn)$.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    $\RR(\Rn)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets.
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    It is sufficient to prove that linear combinations of the functions $e^{\bold{n} \cdot \x}$,
    where $\bold{n} \in \mathbb{Z}^{n}$, are dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets. This fact is a consequence of the Stone-Weierstrass Theorem.
  &lt;/p&gt;
  &lt;p&gt;
    For every compact set $K \subset \Rn$, $C(K)$ is an algebra: it is a vector space, and the product between functions satisfies all the properties listed in the definition of algebra.
    The set $\mathcal{E}_{K} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}, x \in K \}$ is a linear subspace of $C(K)$, therefore it is closed under addition and multiplication by a scalar.
    Thanks to the properties of the exponential function, the product of two elements $f, g, \in \mathcal{E}_{K}$ is still included in $\mathcal{E}_{K}$:
    $$
    f = \sum_{i=1}^s e^{\bold{n}_i \cdot \x} \quad g = \sum_{j=1}^t e^{\bold{m}_j \cdot \x} \Rightarrow f \cdot g = \sum_{i=1}^s \sum_{j=1}^t e^{(\bold{n}_i + \bold{m}_j) \cdot x}.
    $$
    Thus, $\mathcal{E}_{K}$ is a subalgebra of $C(K)$.
  &lt;/p&gt;
  &lt;p&gt;
    Putting $\bold{n} = \bold{0}$, we obtain
    the non-zero constant function $e^{\bold{0} \cdot \x} = 1$.
    Moreover $\mathcal{E}_{K}$ separates points. Let $\bold{y} \neq \bold{z} \in \Rn$, then
    there exists a coordinate $h \in \{1, \mathellipsis, n\}$ such that $y_h \neq z_h$.
    The function $e^{\bold{e}_h \cdot \x}$, where $\bold{e}_h = (0, \mathellipsis, 0, 1, 0, \mathellipsis, 0)$
    is $h$-th vector of the canonical basis, separates $\bold{y}$ from $\bold{z}$
    $$e^{\bold{e}_h \cdot y} = e^{y_h} \neq e^{z_h} = e^{\bold{e}_h \cdot z}.$$
  &lt;/p&gt;
  &lt;p&gt;
    All the hypotheses of Stone-Weierstrass theorem are satisfied. Thus, for every compact $K$, $\mathcal{E}_{K}$
    is dense in $C(K)$ with respect to the uniform norm. That is equivalent to say that $\mathcal{E} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}\}$ is dense in $C(\Rn)$ in the topology of
    uniform convergence on compact subsets.
    &lt;span style=&quot;float:right&quot;&gt;$\square$&lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
    The density of $\RR(\Rn)$ in $C(\Rn)$ tells us that for every continuous functions $f$ it exists a sequence
    of functions in $\RR(\Rn)$ whose limit is $f$ in the chosen topology.
    Thus, any continuous function can be approximated by functions in $\RR(\Rn)$ with an arbitrary small
    approximation error on every compact of $\Rn$.
    &lt;small&gt;
      The uniform convergence on compact sets is not equivalent to uniform convergence.
      In practice, that difference is irrelevant as we always approximate functions whose domain can be included in a compact set. Most of the time, the approximated function is known only on a finite set of points.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    We have already observed that every $\RR(\Omega)$ only depends on the set of normalized directions in $\Omega$. By consequence, the theorem implies that $\RR([-M, M]^n)$ is dense in $C(\Rn)$ for an arbitrary constant $M$.
    In general, we can bound the norm of ridge function directions without any loss in expressive power.
  &lt;/p&gt;
  &lt;p&gt;
    The theorem implies nothing about MLPs. A negative result would have been valid for any subset of $\RR(\Rn)$, while such a positive result only give us an idea of what kind of approximation the MLPs could attain and under which conditions.
  &lt;/p&gt;
  &lt;p&gt;
    A more general theorem by Vostrecov and Kreines states a necessary and sufficient condition
    for the density of $\RR(\Omega)$ in $C(\Rn)$.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    $\RR(\Omega)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets,
    if and only if there is no homogeneous polynomial $p \neq 0$ such that $p(\x) = 0 \ \forall\, \x \in \Omega$.
  &lt;/p&gt;
  &lt;p&gt;
    This final theorem provides the complete characterization of the density of linear combination of ridge functions. We have not discussed if all these properties are preserved by MLPs, but this is a solid starting point to deepen the subject.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
    &lt;p&gt;
      Single hidden layer MLPs are a particular linear combination of ridge functions.
      We have proved that the class of functions spanned by continuous ridge functions approximates every continuous function with arbitrary precision.
      This fact is relevant in applications because it allows designing parametric models without imposing compelling constraints.
    &lt;/p&gt;
    &lt;p&gt;
      The density of $\RR(\Omega)$ in $C(\Rn)$ does not imply that the same property holds for MLPs, but we can hope that functions represented by MLPs have the same approximation capability.
      Actually, like the linear combinations of ridge functions, MLPs are universal approximators.
      That will be the topic of a follow-up article.
    &lt;/p&gt;
&lt;/section&gt;
&lt;footer style=&quot;text-align: left&quot;&gt;
  &lt;section&gt;
    &lt;h1&gt;References&lt;/h1&gt;
    &lt;ol&gt;
      &lt;li id=&quot;ref-1&quot;&gt;
        Pinkus, Allan. &lt;em&gt;Ridge functions.&lt;/em&gt; Vol. 205. Cambridge University Press, 2015.
      &lt;/li&gt;
      &lt;li id=&quot;ref-2&quot;&gt;
        Pinkus, Allan. &lt;em&gt;Approximation theory of the MLP model in neural networks.&lt;/em&gt; Acta numerica 8 (1999): 143-195.
      &lt;/li&gt;
      &lt;li id=&quot;ref-3&quot;&gt;
        Vostrecov, B. A., and M. A. Kreines. &lt;em&gt;Approximation of continuous functions by superpositions of plane waves.&lt;/em&gt; Dokl. Akad. Nauk SSSR. Vol. 140. 1961.
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/section&gt;
  &lt;section&gt;
    &lt;h1&gt;Updates and Corrections&lt;/h1&gt;
    &lt;p&gt;If you see mistakes or want to suggest changes, please &lt;a href=&quot;https://github.com/lucagrementieri/lucagrementieri.github.io&quot;&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt;
  &lt;/section&gt;
&lt;/footer&gt;</content><author><name>Luca Grementieri</name></author><summary type="html">$ \gdef\x{\bold{x}} \gdef\w{\bold{w}} \gdef\a{\bold{a}}</summary></entry><entry><title type="html">Why convolutions are everywhere</title><link href="http://localhost:4000/2020-04-05/convolutions" rel="alternate" type="text/html" title="Why convolutions are everywhere" /><published>2020-04-05T00:00:00+02:00</published><updated>2020-04-05T00:00:00+02:00</updated><id>http://localhost:4000/2020-04-05/convolutions</id><content type="html" xml:base="http://localhost:4000/2020-04-05/convolutions">&lt;section&gt;
  &lt;h1 id=&quot;signals&quot;&gt;Signals&lt;/h1&gt;
  &lt;p&gt;
    There are many kinds of data: images, sounds, text, time series are very well-known examples. All these data are discrete. In some cases, this property is intrinsic: for example, written text is naturally a sequence of characters and words. In most cases, when data are the result of a measurement, analog signals are digitized during the acquisition process.
  &lt;/p&gt;
  &lt;p&gt;
    The devices that perform the measurements are called sensors. Sensors produce discrete signals by &lt;em&gt;sampling&lt;/em&gt; or &lt;em&gt;aggregating&lt;/em&gt; information from the analog world around us.
    For example, a sound recorder &lt;em&gt;samples&lt;/em&gt; audio waves to register a discretized version of them. On the other hand, a camera collects photons in every pixel &lt;em&gt;aggregating&lt;/em&gt; the light coming from all directions in a short amount of time.
  &lt;/p&gt;
  &lt;p&gt;
    From a mathematical point of view, a signal is any function defined on a discrete set. This definition is very general, and it includes all the examples above.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    A discrete &lt;strong&gt;signal&lt;/strong&gt; is a function whose domain is $\mathbb{Z}$,
    $f: \mathbb{Z} \longrightarrow \mathbb{R}$.&lt;/br&gt;
    We denote the $n$-th value of the function as $f[n]$.
    &lt;small&gt;
      The definition provided is very similar to the one of a sequence. We will use this parallelism to derive some theorems and properties of signals. In particular, we will use $\ell^{p}(\mathbb{R})$ spaces.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
      A very particular signal is the discrete Dirac delta.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    The discrete &lt;strong&gt;Dirac delta&lt;/strong&gt; is the signal
    $\delta[n] =
    \begin{cases}
    1 &amp; \text{if } n = 0 \\
    0 &amp; \text{if } n \neq 0
    \end{cases}$.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;discrete-convolution&quot;&gt;Discrete convolution&lt;/h1&gt;
  &lt;p&gt;
    The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real signals, the &lt;strong&gt;discrete convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}.
    $$
  &lt;/p&gt;
  &lt;p&gt;
    The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$.
    When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$.
    &lt;small&gt;
      The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: &lt;/br&gt;
      $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    In general, the summation in the convolution definition is not always finite.
    Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals
    with $p, q \in [1, \infty]$ such that
    $$
    \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\},
    $$
    then $f * g$ is bounded.
    &lt;small&gt;
      For every $p &gt; 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences,
      i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if
      $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p}&lt; +\infty$.&lt;/br&gt;
      $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that
    $$
    |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}.
    $$
    Using Hölder's inequality and with a simple change of variable, we prove the theorem:
    &lt;small&gt;
    Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$
    if $p$ and $q$ satisfy the condition of the theorem.
    The special case $p=q=2$ gives the Cauchy–Schwarz inequality.
    &lt;/small&gt;
    $$
    \begin{aligned}
        |(f*g)[n]| &amp; \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\
        &amp; \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} &amp; \textit{\footnotesize Hölder's ineq.} \\
        &amp; = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} &amp; {\footnotesize k' = n - k } \\
        &amp; = ||f||_{p}||g||_{q}. &amp; \square
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p&gt;
    Two remarkable properties of the convolution are commutativity and linearity.
    These two properties are sufficient to explain why convolutions are a fundamental component
    of many processing systems.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g$ be two real signals such that its convolution is well-defined, then
    $$
    (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    That means that the convolution is a &lt;strong&gt;commutative&lt;/strong&gt; operation.
  &lt;/p&gt;
  &lt;h3&gt;Proof&lt;/h3&gt;
  &lt;p&gt;
    For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$.
    $$
    \begin{aligned}
    (f * g)[n] &amp; = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\
    &amp; = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n]. &amp; \hspace*{6em} \square
    \end{aligned}
    $$
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
    Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below
    are all well-defined, then we have
    $$
    ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    Thus the convolution is a &lt;strong&gt;linear&lt;/strong&gt; operation.
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators&quot;&gt;Operators&lt;/h1&gt;
    &lt;p&gt;
        To extract information from a stream of data, we have to process it.
        Every operation applied to a signal is associated with an operator.
        For example, we can use operators to reduce noise, to recognize features,
        to detect peaks or discontinuities.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is &lt;strong&gt;linear&lt;/strong&gt; if $\forall\: a, b \in \mathbb{R}$
      $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
      A discrete operator is &lt;strong&gt;shift-invariant&lt;/strong&gt; if $\forall\: p \in \mathbb{Z}$
      $$L(f[n-p]) = L(f)[n-p].$$
    &lt;/p&gt;
    &lt;p&gt;
      When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant.
      The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters.
    &lt;/p&gt;
    &lt;p&gt;
      This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;operators-convolution&quot;&gt;Operators and convolution&lt;/h1&gt;
    &lt;p&gt;
      There is a tight relationship between linear shift-invariant operators and convolution.
      To reveal this connection, we need to recall the discrete Dirac.
      Translating the discrete Dirac by $p$, we obtain
      $$
      \delta[n - p] =
      \begin{cases}
      1 &amp; n = p \\
      0 &amp; n \neq p
      \end{cases}.
      $$
      Using this expression, we can represent every signal as a linear combination of Dirac signals
      $$
      f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
      $$
      Now we have all the elements necessary to prove the main theorem of the article.
    &lt;/p&gt;
    &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
      &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;THEOREM&lt;/strong&gt;&lt;/br&gt;
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    &lt;/p&gt;
    &lt;h3&gt;Proof&lt;/h3&gt;
    &lt;p&gt;
        If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$.
        Setting $h[n] = L(\delta)[n]$, we have
        $$
        \begin{aligned}
        L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\
        &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp;
        \end{aligned}
        $$
        To prove the inverse implication, we have to show that for every signal $h[n]$,
        $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution.
        Now, we show that the resulting operation is also shift-invariant.
        $$
        \begin{aligned}
            L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\
            &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\
            &amp; = L(f)[n - p]. &amp; \square
        \end{aligned}
        $$
    &lt;/p&gt;
    &lt;p&gt;
      This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations.
      We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons.
      Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines.
    &lt;/p&gt;
    &lt;p&gt;
      Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
  &lt;h1 id=&quot;generalizations&quot;&gt;Generalizations&lt;/h1&gt;
  &lt;p&gt;
    In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g$ be two real function, the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt  \quad \forall\: x \in \mathbb{R}.
    $$
  &lt;/p&gt;
  &lt;p&gt;
    All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions.
    &lt;small&gt;
      Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can.
    &lt;/small&gt;
  &lt;/p&gt;
  &lt;p&gt;
    Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension.
    We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case.
  &lt;/p&gt;
  &lt;p style=&quot;background-color:#d8deea; padding:10px;&quot;&gt;
    &lt;strong style=&quot;color:#434498; padding-bottom:10px&quot;&gt;DEFINITION&lt;/strong&gt;&lt;/br&gt;
    Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals,
    the &lt;strong&gt;convolution&lt;/strong&gt; of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k]  \quad \forall\: n \in \mathbb{Z}^{d}.
    $$
  &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;cnn&quot;&gt;Convolutional Neural Networks&lt;/h1&gt;
    &lt;p&gt;
      The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision.
      A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling.
    &lt;/p&gt;
    &lt;p&gt;
      Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant.
    &lt;/p&gt;
    &lt;p&gt;
      Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen.
    &lt;/p&gt;
    &lt;p&gt;
      Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation!
      A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper &quot;&lt;a href=https://arxiv.org/abs/1904.11486&gt;Making Convolutional Networks Shift-Invariant Again&lt;/a&gt;&quot;.
    &lt;/p&gt;
&lt;/section&gt;
&lt;section&gt;
    &lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
    &lt;p&gt;
      The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications.
    &lt;/p&gt;
&lt;/section&gt;
&lt;footer style=&quot;text-align: left&quot;&gt;
  &lt;section&gt;
    &lt;h1&gt;References&lt;/h1&gt;
    &lt;ol&gt;
      &lt;li id=&quot;ref-1&quot;&gt;
        Mallat, Stéphane. &lt;em&gt;A wavelet tour of signal processing&lt;/em&gt;. Elsevier, 1999.
      &lt;/li&gt;
      &lt;li id=&quot;ref-2&quot;&gt;
        Zhang, Richard. &lt;em&gt;Making convolutional networks shift-invariant again&lt;/em&gt;. arXiv preprint arXiv:1904.11486 (2019).
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/section&gt;
  &lt;section&gt;
    &lt;h1&gt;Updates and Corrections&lt;/h1&gt;
    &lt;p&gt;If you see mistakes or want to suggest changes, please &lt;a href=&quot;https://github.com/lucagrementieri/lucagrementieri.github.io&quot;&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt;
  &lt;/section&gt;
&lt;/footer&gt;</content><author><name>Luca Grementieri</name></author><summary type="html">Signals There are many kinds of data: images, sounds, text, time series are very well-known examples. All these data are discrete. In some cases, this property is intrinsic: for example, written text is naturally a sequence of characters and words. In most cases, when data are the result of a measurement, analog signals are digitized during the acquisition process. The devices that perform the measurements are called sensors. Sensors produce discrete signals by sampling or aggregating information from the analog world around us. For example, a sound recorder samples audio waves to register a discretized version of them. On the other hand, a camera collects photons in every pixel aggregating the light coming from all directions in a short amount of time. From a mathematical point of view, a signal is any function defined on a discrete set. This definition is very general, and it includes all the examples above. DEFINITION A discrete signal is a function whose domain is $\mathbb{Z}$, $f: \mathbb{Z} \longrightarrow \mathbb{R}$. We denote the $n$-th value of the function as $f[n]$. The definition provided is very similar to the one of a sequence. We will use this parallelism to derive some theorems and properties of signals. In particular, we will use $\ell^{p}(\mathbb{R})$ spaces. A very particular signal is the discrete Dirac delta. DEFINITION The discrete Dirac delta is the signal $\delta[n] = \begin{cases} 1 &amp; \text{if } n = 0 \\ 0 &amp; \text{if } n \neq 0 \end{cases}$. Discrete convolution The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions. DEFINITION Let $f$, $g$ be two real signals, the discrete convolution of $f$ and $g$ is $$ (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}. $$ The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$. When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$. The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$. In general, the summation in the convolution definition is not always finite. Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element. THEOREM Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals with $p, q \in [1, \infty]$ such that $$ \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\}, $$ then $f * g$ is bounded. For every $p &gt; 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences, i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p} $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences. Proof $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that $$ |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}. $$ Using Hölder's inequality and with a simple change of variable, we prove the theorem: Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$ if $p$ and $q$ satisfy the condition of the theorem. The special case $p=q=2$ gives the Cauchy–Schwarz inequality. $$ \begin{aligned} |(f*g)[n]| &amp; \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\ &amp; \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}} \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} &amp; \textit{\footnotesize Hölder's ineq.} \\ &amp; = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}} \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} &amp; {\footnotesize k' = n - k } \\ &amp; = ||f||_{p}||g||_{q}. &amp; \square \end{aligned} $$ Two remarkable properties of the convolution are commutativity and linearity. These two properties are sufficient to explain why convolutions are a fundamental component of many processing systems. THEOREM Let $f, g$ be two real signals such that its convolution is well-defined, then $$ (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}. $$ That means that the convolution is a commutative operation. Proof For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$. $$ \begin{aligned} (f * g)[n] &amp; = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\ &amp; = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n]. &amp; \hspace*{6em} \square \end{aligned} $$ THEOREM Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below are all well-defined, then we have $$ ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}. $$ Thus the convolution is a linear operation. Operators To extract information from a stream of data, we have to process it. Every operation applied to a signal is associated with an operator. For example, we can use operators to reduce noise, to recognize features, to detect peaks or discontinuities. DEFINITION A discrete operator $L$ is linear if $\forall\: a, b \in \mathbb{R}$ $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$ DEFINITION A discrete operator is shift-invariant if $\forall\: p \in \mathbb{Z}$ $$L(f[n-p]) = L(f)[n-p].$$ When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant. The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters. This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series. Operators and convolution There is a tight relationship between linear shift-invariant operators and convolution. To reveal this connection, we need to recall the discrete Dirac. Translating the discrete Dirac by $p$, we obtain $$ \delta[n - p] = \begin{cases} 1 &amp; n = p \\ 0 &amp; n \neq p \end{cases}. $$ Using this expression, we can represent every signal as a linear combination of Dirac signals $$ f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]. $$ Now we have all the elements necessary to prove the main theorem of the article. THEOREM A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete signal $h[n]$ such that for every signal $f[n]$ $$ L(f)[n] = (f * h)[n]. $$ Proof If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$. Setting $h[n] = L(\delta)[n]$, we have $$ \begin{aligned} L(f)[n] &amp; = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) &amp; \\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) &amp; \textit{\footnotesize linearity} \\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] &amp; \textit{\footnotesize shift-invariance}\\ &amp; = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &amp; \end{aligned} $$ To prove the inverse implication, we have to show that for every signal $h[n]$, $L(f) = f * h$ is a linear shift-invariant operator. The linearity comes from the linearity of the convolution. Now, we show that the resulting operation is also shift-invariant. $$ \begin{aligned} L(f[n-p]) &amp; = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) &amp; \\ &amp; = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) &amp; \qquad {\footnotesize k' = k - p} \\ &amp; = L(f)[n - p]. &amp; \square \end{aligned} $$ This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations. We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons. Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines. Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too. Generalizations In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented. DEFINITION Let $f$, $g$ be two real function, the convolution of $f$ and $g$ is $$ (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt \quad \forall\: x \in \mathbb{R}. $$ All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions. Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can. Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension. We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case. DEFINITION Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals, the convolution of $f$ and $g$ is $$ (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}^{d}. $$ Convolutional Neural Networks The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision. A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling. Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant. Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen. Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation! A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper &quot;Making Convolutional Networks Shift-Invariant Again&quot;. Conclusions The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications. References Mallat, Stéphane. A wavelet tour of signal processing. Elsevier, 1999. Zhang, Richard. Making convolutional networks shift-invariant again. arXiv preprint arXiv:1904.11486 (2019). Updates and Corrections If you see mistakes or want to suggest changes, please open an issue on GitHub.</summary></entry></feed>