<!DOCTYPE html>
<html>
	<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=yes">

  <title>Artificial neurons as ridge functions</title>
  <meta name="description" content="Approximation theory of ridge functions">
  <meta name="keywords" content="Luca Grementieri, Artificial Intelligence, Machine Learning, Mathematics"/>
  <meta name="robots" content="index,follow">

  <meta property="og:url" content="https://lucagrementieri.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Luca Grementieri">
  <meta property="og:description" content="Scientific blog about maths and machine learning.">
  <meta property="og:site_name" content="lucagrementieri.github.io">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="Scientific blog about maths and machine learning.">
  <meta name="twitter:url" content="https://lucagrementieri.github.io/">
  <meta name="twitter:title" content="Luca Grementieri">
  <meta name="twitter:description" content="Scientific blog about maths and machine learning.">

  <meta itemprop="name" content="Luca Grementieri">
  <meta itemprop="description" content="Scientific blog about maths and machine learning.">

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
  <link href="https://unpkg.com/@nextbitlabs/rapido@^3/rapido.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Luca Grementieri" />

  <!-- Google Analytics-->
  

  <style>
  footer {
    font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, sans-serif
  }
  </style>
</head>

	<body>
		<nav class="nav">
  <div class="nav-container">
    <a href="/">
      <h2 class="nav-title">Luca Grementieri</h2>
    </a>
    <ul>
      <li><a href="/about">About</a></li>
      <li><a href="/">Posts</a></li>
      <li style="padding:0;"><a href="https://github.com/lucagrementieri/">
        <img style="margin: 0; max-height:20px;" alt="GitHub" src="/assets/icons/github.png"></a></li>
      <li style="padding:0;"><a href="https://www.linkedin.com/in/luca-grementieri/">
        <img style="margin: 0; max-height:20px;" alt="LinkedIn" src="/assets/icons/linkedin.png"></a></li>
      <li style="padding:0;"><a href="/feed.xml">
        <img style="margin: 0; max-height:20px;" alt="RSS" src="/assets/icons/rss.png"></a></li>
    </ul>
  </div>
</nav>

		<main>
			<article class="rapido" style="padding:0px;">
        <header>
          <h1>Artificial neurons as ridge functions</h1>
          <p>
            Artificial neurons are a special case of ridge functions and linear combinations of ridge functions are a generalization of multilayer perceptrons with one hidden layer. The approximation theory of ridge functions is an important gateway to the study of approximation capabilities of neural networks.
          </p>
        </header>
				<!-- improve intro -->
$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}
\gdef\a{\bold{a}}

\gdef\R{\mathbb{R}}
\gdef\RR{\mathcal{R}}
\gdef\Rn{\mathbb{R}^{n}}
\gdef\M{\mathcal{M}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
\gdef\max#1{\underset{#1}{\text{max}}}
$
<section>
  <h1 id="approximation-theory">Approximation theory</h1>
  <p>
    Approximation theory is the branch of mathematics that studies how to approximate a class of functions
    with a set of simpler functions. It allows to discover set of functions described by finite or countable
    parameters that can faithfully represent an uncountable class of functions.
    This subject has great practical interest because it allows to solve optimization problems on functions without making strong assumptions on the searched function.
  </p>
  <p>
    Given a functional $\ell: \mathcal{F} \longrightarrow \R$,
    an optimization problem takes the form
    $$
    f^{*} = \argmin{f \in \mathcal{F}}\ \ell(f),
    $$
    where $\mathcal{F}$ is the search space of functions determined by the assumptions on $f^{*}$.
    For example, if we know that the solution $f^{*}$ is a continuous univariate function, then $\mathcal{F} = C(\R)$.
  </p>
  <p>
    In practice we cannot solve the optimization problem on a function space with an infinite number of dimensions,
    because machines can work only with a finite number of parameters. So we need to restrict $\mathcal{F}$ to a more
    manageable subset $\mathcal{G} \subset \mathcal{F}$.
    This restriction adds more assumptions on $f^{*}$, assumptions not dictated by the problem itself but by
    computational requirements. For example, in many cases, the search space is restricted to linear functions.
  </p>
  <p>
    Approximation theory looks for a countable or uncountable $\mathcal{G}$ dense in $\mathcal{F}$.
    The density property assures that the effective search space is not reduced because $f^{*}$ can be described
    by functions in $\mathcal{G}$ with arbitrary precision.
    Let us recall the definition of density.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $X$ be a topological space and let $A \subseteq X$. $A$ is <strong>dense</strong> in $X$
    if for every $x \in X$, any neighborhood of $x$ contains at least one point from $A$.
    Equivalently, $A$ is dense in $X$ if the closure of $A$ is $X$: $\overline{A} = X$.
    <!-- add note about neighborhood and balls-->
  </p>
  <p>
    The most known result of approximation theory is <strong>Weierstrass Approximation Theorem</strong>.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f$ be a continuous real-valued function defined on the interval $[a, b]$.
    For every $\epsilon > 0$, there exists a polynomial $p$ such that $\forall\, x \in [a, b]$, we have
    $|f(x) − p(x)| < \epsilon$, or equivalently, $||f−p||_{\infty} < \epsilon$.
    <!-- add note about supremum norm-->
  </p>
</section>
<section>
  <h1 id="mlp">Multilayer perceptron</h1>
  <p>
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of <em>layers</em>.
    Every layer is composed by <em>neurons</em>, the fundamental processing unit of the network.
    Every artificial neuron in the intermediate <em>hidden layers</em> processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the <em>activation function</em> $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    <small>
      In the neural network literature, the values $w_{i}$ are called <em>weights</em> and they model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron. $\theta$ is called <em>bias</em> and it recalls the threshold potential involved in the firing of biologic neurons.
    </small>
  </p>
  <p>
    In the final layer, said the <em>output layer</em>, neurons operates differently. They just perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this model because it can already approximate a very large class of functions.
    <!-- add small to explain that we are dealing with regression but classification is shaped as a regression too-->
  </p>
</section>
<section>
  <h1 id="ridge">Ridge functions</h1>
  <p>
    The function $N(\x)$ has a particular form: it is the composition of a univariate function $\sigma$ with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>ridge function</strong> is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  </p>
  <p>
    The vector $\a$ is called the <em>direction</em> because a ridge function is a multivariate function constant on the parallel hyperplanes orthogonal to $\a$. In fact, these hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  </p>
  <p>
    For a given direction $\a$, we denote the set of ridge functions with direction $\a$
    $$\RR(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\RR(\a) = \RR(\bold{b})$.
  </p>
  <p>
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \RR(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A <em>span</em> of a set $S$ is the set of linear combinations of elements of $S$, then
    every $F \in \RR(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\RR(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
  </p>
  <p>
    If we require the continuity of the activation function $\sigma$ in a MLP, the functions defined by a MLP
    belong to the smaller set of linear combinations of continuous ridge functions
    $$
    \M(\Omega) = \text{span}\{f(\a \cdot \x), f \in C(\R), \a \in \Omega\}
    $$
    with $\Omega = \Rn$.
    We are interested in the theory of approximation of sets $\M(\Omega)$ to better characterize the class of functions defined by MLPs.
  </p>
</section>
<section>
  <h1 id="compact-convergence">Uniform convergence on compact sets</h1>
  <p>
    $\M(\Omega)$ is a large class of continuous functions. We are going to find the conditions
    on $\Omega$ that assure that it is dense in the set of continuous functions $C(\Rn)$.
    Since density is a property of a topological space, we have to define a topology
    on $C(\Rn)$ to clarify the meaning of this statement.
    Approximation theory of continuous functions commonly use the topology of uniform
    converge on compact subsets.
    <!-- note on compact sets -->
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A sequence of functions $f_{m} \in C(\Rn), m \in \mathbb{N}$ is said to
    <strong>converge uniformly on compact sets</strong> as $m \to \infty$ to some function
    $f \in C(\Rn)$ if, for every compact set $K \subset \Rn$ and every $\epsilon > 0$,
    there exist $\overline{m}$ such that $\forall\, m \geq \overline{m}$
    $$
    ||f_{m} - f||_{K} = \max{x \in K}\ | f_{m}(x) - f(x)| < \epsilon.
    $$
    <small>
      If we can prove density in this topology, then we also obtain density in many other topologies.
      For example, density will hold in $L^{p}(K)$, where $K$ is any compact subset of $\Rn$ and $p \in [1,\infty[$.
    </small>
  </p>
  <p>
    A very powerful tool to prove results with this topology is the <strong>Stone-Weierstrass Theorem</strong>.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Suppose $X$ is a compact Hausdorff space and $A$ is a subalgebra of $C(X, \R)$ which contains a non-zero constant function. Then $A$ is dense in $C(X, \R)$ if and only if it separates points.
    <small>
      This theorem implies Weierstrass Approximation Theorem since the polynomials on $[a, b]$ form a subalgebra of $C([a, b])$ which contains the constants and separates points.
    </small>
  </p>
  <!-- definition of sub-algebra -->
</section>
<section>
  <h1 id="compact-convergence">Density of continuous ridge functions</h1>
  <p>
    We now have all the ingredient to state a density theorem about $\M(\Rn)$ in $C(\Rn)$.
    In reality the theorem is stronger because it proves that $M(\mathbb{Z}^{n})$ is dense in $C(\Rn)$.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    $\M(\Rn)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets.
  </p>
  <h3>Proof</h3>
  <p>
    It is sufficient to prove that linear combinations of the functions $e^{\bold{n} \cdot \x}$,
    where $\bold{n} \in \mathbb{Z}^{n}$ are dense in $C(\Rn)$, in the topology of uniform convergence on compact subsets. This fact is an immediate consequence of the Stone-Weierstrass Theorem.
    When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant.
    The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters.
  </p>
</section>
<section>
    <h1 id="operators-convolution">Operators and convolution</h1>
    <p>
      There is a tight relationship between linear shift-invariant operators and convolution.
      To reveal this connection, we need to recall the discrete Dirac.
      Translating the discrete Dirac by $p$, we obtain
      $$
      \delta[n - p] =
      \begin{cases}
      1 & n = p \\
      0 & n \neq p
      \end{cases}.
      $$
      Using this expression, we can represent every signal as a linear combination of Dirac signals
      $$
      f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
      $$
      Now we have all the elements necessary to prove the main theorem of the article.
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    </p>
    <h3>Proof</h3>
    <p>
        If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$.
        Setting $h[n] = L(\delta)[n]$, we have
        $$
        \begin{aligned}
        L(f)[n] & = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) & \\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) & \textit{\footnotesize linearity} \\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] & \textit{\footnotesize shift-invariance}\\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &
        \end{aligned}
        $$
        To prove the inverse implication, we have to show that for every signal $h[n]$,
        $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution.
        Now, we show that the resulting operation is also shift-invariant.
        $$
        \begin{aligned}
            L(f[n-p]) & = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) & \\
            & = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) & \qquad {\footnotesize k' = k - p} \\
            & = L(f)[n - p]. & \square
        \end{aligned}
        $$
    </p>
    <p>
      This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations.
      We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons.
      Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines.
    </p>
    <p>
      Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too.
    </p>
</section>
<section>
  <h1 id="generalizations">Generalizations</h1>
  <p>
    In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g$ be two real function, the <strong>convolution</strong> of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt  \quad \forall\: x \in \mathbb{R}.
    $$
  </p>
  <p>
    All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions.
    <small>
      Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can.
    </small>
  </p>
  <p>
    Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension.
    We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals,
    the <strong>convolution</strong> of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k]  \quad \forall\: n \in \mathbb{Z}^{d}.
    $$
  </p>
</section>
<section>
    <h1 id="cnn">Convolutional Neural Networks</h1>
    <p>
      The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision.
      A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling.
    </p>
    <p>
      Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant.
    </p>
    <p>
      Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen.
    </p>
    <p>
      Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation!
      A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper "<a href=https://arxiv.org/abs/1904.11486>Making Convolutional Networks Shift-Invariant Again</a>".
    </p>
</section>
<section>
    <h1 id="conclusions">Conclusions</h1>
    <p>
      The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications.
    </p>
</section>
<footer style="text-align: left">
  <section>
    <h1>References</h1>
    <ol>
      <li id="ref-1">
        Mallat, Stéphane. <em>A wavelet tour of signal processing</em>. Elsevier, 1999.
      </li>
      <li id="ref-2">
        Zhang, Richard. <em>Making convolutional networks shift-invariant again</em>. arXiv preprint arXiv:1904.11486 (2019).
      </li>
    </ol>
  </section>
  <section>
    <h1>Updates and Corrections</h1>
    <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/lucagrementieri/lucagrementieri.github.io">open an issue on GitHub</a>.</p>
  </section>
</footer>

			</article>
		</main>
		<footer>
  <span>
    &copy; <time datetime="2020-04-19 20:13:47 +0200">2020</time> Luca Grementieri. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme and
    <a href="https://github.com/nextbitlabs/Rapido">Rapido</a>.
  </span>
</footer>

	</body>
</html>
