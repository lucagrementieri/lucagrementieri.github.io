<!DOCTYPE html>
<html>
	<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=yes">

  <title>Artificial neurons as ridge functions</title>
  <meta name="description" content="Approximation theory of ridge functions">
  <meta name="keywords" content="Luca Grementieri, Artificial Intelligence, Machine Learning, Mathematics"/>
  <meta name="robots" content="index,follow">

  <meta property="og:url" content="https://lucagrementieri.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Luca Grementieri">
  <meta property="og:description" content="Scientific blog about maths and machine learning.">
  <meta property="og:site_name" content="lucagrementieri.github.io">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="Scientific blog about maths and machine learning.">
  <meta name="twitter:url" content="https://lucagrementieri.github.io/">
  <meta name="twitter:title" content="Luca Grementieri">
  <meta name="twitter:description" content="Scientific blog about maths and machine learning.">

  <meta itemprop="name" content="Luca Grementieri">
  <meta itemprop="description" content="Scientific blog about maths and machine learning.">

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload="renderMathInElement(document.body,{delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
  <link href="https://unpkg.com/@nextbitlabs/rapido@^3/rapido.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Luca Grementieri" />

  <!-- Google Analytics-->
  

  <style>
  footer {
    font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, sans-serif
  }
  </style>
</head>

	<body>
		<nav class="nav">
  <div class="nav-container">
    <a href="/">
      <h2 class="nav-title">Luca Grementieri</h2>
    </a>
    <ul>
      <li><a href="/about">About</a></li>
      <li><a href="/">Posts</a></li>
      <li style="padding:0;"><a href="https://github.com/lucagrementieri/">
        <img style="margin: 0; max-height:20px;" alt="GitHub" src="/assets/icons/github.png"></a></li>
      <li style="padding:0;"><a href="https://www.linkedin.com/in/luca-grementieri/">
        <img style="margin: 0; max-height:20px;" alt="LinkedIn" src="/assets/icons/linkedin.png"></a></li>
      <li style="padding:0;"><a href="/feed.xml">
        <img style="margin: 0; max-height:20px;" alt="RSS" src="/assets/icons/rss.png"></a></li>
    </ul>
  </div>
</nav>

		<main>
			<article class="rapido" style="padding:0px;">
        <header>
          <h1>Artificial neurons as ridge functions</h1>
          <p>
            Artificial neurons are a particular case of ridge functions. Thus, linear combinations of ridge functions are a generalization of multilayer perceptrons with one hidden layer. Approximation theory of ridge functions provides an upper bound on the approximation capabilities of neural networks.
          </p>
        </header>
				$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}
\gdef\a{\bold{a}}

\gdef\R{\mathbb{R}}
\gdef\RR{\mathcal{R}}
\gdef\Rn{\mathbb{R}^{n}}
\gdef\M{\mathcal{M}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
\gdef\max#1{\underset{#1}{\text{max}}}
$
<section>
  <h1 id="approximation-theory">Approximation theory</h1>
  <p>
    Approximation theory is the branch of mathematics that studies how to approximate a large class of functions combining simpler ones.
    It looks for sets of functions described by a finite or countable number of parameters that can faithfully reproduce functional classes with an uncountable number of dimensions. Continuous mappings are the most studied example of such a large family.
  </p>
  <p>
    This topic has a great practical interest in solving optimization problems on functions without making strong hypotheses on the optimal solution.
    Given a functional $\ell: \mathcal{F} \longrightarrow \R$,
    an optimization problem takes the form
    $$
    f^{*} = \argmin{f \in \mathcal{F}}\ \ell(f),
    $$
    where $\mathcal{F}$ is the search space of functions determined by the assumptions on $f^{*}$.
    For example, if we know that the solution $f^{*}$ is a continuous univariate function, then $\mathcal{F} = C(\R)$.
  </p>
  <p>
    In practice, we cannot solve the optimization problem on a function space with an infinite number of dimensions, because machines can work only with a finite number of parameters. So we need to restrict $\mathcal{F}$ to a more
    manageable subset $\mathcal{G} \subset \mathcal{F}$.
    This restriction adds more assumptions on f. Computational requirements dictates these assumptions, not the problem itself. For instance, in many cases, the search space is restricted to linear functions because of their simplicity and interpretability.
  </p>
  <p>
    Representing uncountable dimensions with a countable amount of them seems impossible. The density property makes it possible. We use this same property in every calculation on a computer.
    Since $\mathbb{Q}$ is dense in $\R$, we can represent real numbers on machines with a precision that is only limited by the available memory.
  </p>
  <p>
    Similarly, the approximation theory of functions looks for a countable set $\mathcal{G}$ dense in $\mathcal{F}$.
    Density assures that the actual search space is not reduced because functions in $\mathcal{G}$ can reproduce $f^{*}$ with arbitrary precision. Let us recall the definition of a dense set.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $X$ be a topological space and let $A \subseteq X$. $A$ is <strong>dense</strong> in $X$
    if for every $x \in X$, any neighborhood of $x$ contains at least one point from $A$.
    Equivalently, $A$ is dense in $X$ if the closure of $A$ is $X$: $\overline{A} = X$.
    <small>
      The definitions of neighborhood and closure come from general topology. A neighborhood of a point $x$ is a set that includes an open subset containing $x$. The closure of a set $S$ is the smallest closed set that covers $S$. It is the union of $S$ and its limit points.
    </small>
  </p>
  <p>
    The most known result of approximation theory is <strong>Weierstrass Approximation Theorem</strong>.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f$ be a continuous real-valued function defined on the interval $[a, b]$.
    For every $\epsilon > 0$, there exists a polynomial $p$ such that $\forall\, x \in [a, b]$, we have
    $|f(x) − p(x)| < \epsilon$, or equivalently, $||f−p||_{\infty} < \epsilon$.
    <small>
      The notation $||\cdot||_{\infty}$ indicates the uniform or supremum norm.
      The norm assigns to a bounded function $f$ on the interval $[a, b]$ the value
      $||f||_{\infty} = \text{sup}\{|f(x)|, x \in [a, b]\}$.
    </small>
  </p>
</section>
<section>
  <h1 id="mlp">Multilayer perceptron</h1>
  <p>
    Neural networks encode a subset of continuous functions. To examine the approximation capabilities of this class, we introduce the multiplayer perceptron and one of its generalization.
  </p>
  <p>
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of <em>layers</em>.
    Every layer is composed by <em>neurons</em>, the elementary processing units of the network.
    Every artificial neuron in the intermediate <em>hidden layers</em> processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the <em>activation function</em> $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    <small>
      In scientific literature about neural networks, the values $w_{i}$ are called <em>weights</em>. They model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron.
      $\theta$, called <em>bias</em>, recalls the threshold potential involved in the firing of biologic neurons.
    </small>
  </p>
  <p>
    In the final layer, said the <em>output layer</em>, neurons operate differently. They perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this shallow model because it can already approximate a large class of functions.
    <small>
      The considered MLP model outputs a single real number.
      The analysis can be generalized to outputs in $\R^{m}$ by repeating it on every component.
      A model whose output is multi-dimensional can be used for both regression and classification tasks. Indeed, classification problems are framed as a regression of class probabilities.
    </small>
  </p>
</section>
<section>
  <h1 id="ridge">Ridge functions</h1>
  <p>
    The function $N(\x)$ has a particular form: it is the composition of a univariate function $\sigma$ with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>ridge function</strong> is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  </p>
  <p>
    The vector $\a$ is called the <em>direction</em> because a ridge function is a multivariate function constant on the parallel hyperplanes orthogonal to $\a$. In fact, these hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  </p>
  <p>
    For a given direction $\a$, we denote the set of ridge functions with direction $\a$
    $$\RR(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\RR(\a) = \RR(\bold{b})$.
  </p>
  <p>
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \RR(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A <em>span</em> of a set $S$ is the set of linear combinations of elements of $S$, then
    every $F \in \RR(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\RR(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
  </p>
  <p>
    If we require the continuity of the activation function $\sigma$ in a MLP, the functions defined by a MLP
    belong to the smaller set of linear combinations of continuous ridge functions
    $$
    \M(\Omega) = \text{span}\{f(\a \cdot \x), f \in C(\R), \a \in \Omega\}
    $$
    with $\Omega = \Rn$.
  </p>
  <p>
    We are interested in the theory of approximation of sets $\M(\Omega)$
    to better characterize the class of functions defined by a MLP.
    Functions well approximated by a MLPs are a subset of the functions
    approximated by $\M(\Rn)$, therefore $\M(\Rn)$ gives us an upper bound on
    the approximation capabilities of a MLP.
  </p>
</section>
<section>
  <h1 id="compact-convergence">Uniform convergence on compact sets</h1>
  <p>
    $\M(\Omega)$ is a large class of continuous functions. We are going to find the conditions
    on $\Omega$ that assure that it is dense in the set of continuous functions $C(\Rn)$.
    Since density is a property of a topological space, we have to define a topology
    on $C(\Rn)$ to clarify the meaning of this statement.
    Approximation theory of continuous functions commonly use the topology of uniform
    converge on compact subsets.
    <small>
      The general topological definition of a compact set involve covers and subcover.
      The Heine-Borel theorem gives a much simpler characterization of compactness in an euclidean space:
      in $\Rn$ a set is compact if and only if it is closed and bounded.
    </small>
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A sequence of functions $f_{m} \in C(\Rn), m \in \mathbb{N}$ is said to
    <strong>converge uniformly on compact sets</strong> as $m \to \infty$ to some function
    $f \in C(\Rn)$ if, for every compact set $K \subset \Rn$ and every $\epsilon > 0$,
    there exist $\overline{m}$ such that $\forall\, m \geq \overline{m}$
    $$
    ||f_{m} - f||_{K} = \max{x \in K}\ | f_{m}(x) - f(x)| < \epsilon.
    $$
    <small>
      If we can prove density in this topology, then the subset is also dense in many other topologies.
      For example, density will hold in $L^{p}(K)$, where $K$ is any compact subset of $\Rn$ and $p \in [1,\infty[$.
      Indeed $||f||_{p,K} \leq ||f||_{K} < +\infty$.
    </small>
  </p>
  <p>
    A very powerful tool to prove results with this topology is the <strong>Stone-Weierstrass Theorem</strong>.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Suppose $X$ is a compact Hausdorff space and $A$ is a subalgebra of $C(X)$, the space of continuous real-valued functions over $X$, which contains a non-zero constant function. Then $A$ is dense in $C(X)$ if and only if it separates points.
    <small>
      This theorem implies Weierstrass Approximation Theorem since the polynomials on $[a, b]$ form a subalgebra of $C([a, b])$ which contains the constants and separates points.
    </small>
  </p>
  <p>
    Let us recall the definition of algebra and subalgebra.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $A$ be a vector space over $\R$ equipped with a binary operation (a product) from $\cdot: A \times A \to A$. Then $A$ is an <strong>algebra</strong> if the following identities hold for all elements $x$, $y$, and $z$ of $A$, and all scalars $a$ and $b$ of $\R$:
    $$
    \begin{aligned}
    (x + y) \cdot z &= x \cdot z + y \cdot z; & \qquad \textit{\footnotesize right distributivity}\\
    z \cdot (x + y) &= z \cdot x + z \cdot y; & \qquad \textit{\footnotesize left distributivity}\\
    (ax) \cdot (by) &= (ab) (x \cdot y). & \qquad \textit{\footnotesize compatibility with scalars}
    \end{aligned}
    $$
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>subalgebra</strong> is a subset of an algebra, closed under addition, multiplication by scalar and
    product.
  </p>
</section>
<section>
  <h1 id="compact-convergence">Density of continuous ridge functions</h1>
  <p>
    We now have all the ingredient to state a density theorem about $\M(\Rn)$ in $C(\Rn)$.
    In reality the theorem is stronger because it proves that $\M(\mathbb{Z}^{n})$ is dense in $C(\Rn)$.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    $\M(\Rn)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets.
  </p>
  <h3>Proof</h3>
  <p>
    It is sufficient to prove that linear combinations of the functions $e^{\bold{n} \cdot \x}$,
    where $\bold{n} \in \mathbb{Z}^{n}$ are dense in $C(\Rn)$, in the topology of uniform convergence on compact subsets. This fact is a consequence of the Stone-Weierstrass Theorem.
  </p>
  <p>
    For every compact set $K \subset \Rn$, $C(K)$ is an algebra: it is a vector space and the product between functions satisfies all the properties
    listed in the definition of algebra.
    The set $\mathcal{E}_{K} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}, x \in K \}$ is a linear subspace of $C(K)$, therefore it is closed under addition and multiplication by a scalar.
    Thanks to the properties of the exponential function, the product of two elements $f, g, \in \mathcal{E}_{K}$ is still included in $\mathcal{E}_{K}$:
    $$
    f = \sum_{i=1}^s e^{\bold{n}_i \cdot \x} \quad g = \sum_{j=1}^t e^{\bold{m}_j \cdot \x} \Rightarrow f \cdot g = \sum_{i=1}^s \sum_{j=1}^t e^{(\bold{n}_i + \bold{m}_j) \cdot x}.
    $$
    Thus, $\mathcal{E}_{K}$ is a subalgebra of $C(K)$. Putting $\bold{n} = \bold{0}$, we obtain
    the non-zero constant function $e^{\bold{0} \cdot \x} = 1$.
    Moreover $\mathcal{E}_{K}$ separates points. Let $\bold{y} \neq \bold{z} \in \Rn$, then
    there exists a coordinate $h \in \{1, \mathellipsis, n\}$ such that $y_h \neq z_h$.
    The function $e^{\bold{e}_h \cdot \x}$, where $\bold{e}_h = (0, \mathellipsis, 0, 1, 0, \mathellipsis, 0)$
    is $h$-th vector of the canonical basis, separates $\bold{y}$ from $\bold{z}$ since
    $$e^{\bold{e}_h \cdot y} = e^{y_h} \neq e^{z_h} = e^{\bold{e}_h \cdot z}.$$
    All the hypotheses of Stone-Weierstrass theorem are satisfied, so for every compact $K$, $\mathcal{E}_{K}$
    is dense in $C(K)$ with respect to the uniform norm. That is equivalent to say that $\mathcal{E} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}\}$ is dense in $C(\Rn)$ in the topology of
    uniform convergence on compact subsets.
    <span style="float:right">$\square$</span>
  </p>
  <p>
    The density of $\M(\Omega)$ in $C(\Rn)$ tells us that for every continuous functions $f$ it exists a sequence
    of functions in $\M(\Omega)$ whose limit is $f$ in the chosen topology.
    Thus, any continuous function can be approximated by functions in $\M(\Omega)$ with an arbitrary small
    approximation error.
    <small>
      The uniform convergence on compact sets is not equivalent to uniform convergence.
      In practice, it does not matter because we always approximate functions whose domain can be included in a compact set; in fact, the function to be approximated is often known only on a finite set of points.
    </small>
  </p>
  <p>
    Since $\M(\Omega)$ only depends on the set of normalized directions in $\Omega$, the
    theorem also implies that $\M([-M, M]^n)$ is dense in $C(\Rn)$ for an arbitrary constant $M$.
    In general, we can safely reduce the space of directions of ridge functions,
    for example limiting their norm, without losing expressive power.
  </p>
  <p>
    A more general theorem by Vostrecov and Kreines states a necessary and sufficient condition
    for the density of $\M(\Omega)$ in $C(\Rn)$.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    $\M(\Omega)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets,
    if and only if there is no homogeneous polynomial $p \neq 0$ such that $p(\x) = 0 \ \forall\, \x \in \Omega$.
  </p>
</section>
<section>
    <h1 id="conclusions">Conclusions</h1>
    <p>
      Single hidden layer MLPs are a special case of a linear combination of ridge functions.
      We have proved that the class of functions spanned by continuous ridge function can approximate
      every continuous function with arbitrary precision.
      This fact is relevant in applications because it allows to design parametric models without imposing
      compelling constraints.
    </p>
    <p>
      This density result does not imply that the same property holds true for MLPs, but
      we can hope that functions represented by a MLP have the same approximation capability (indeed they have).
    </p>
</section>
<footer style="text-align: left">
  <section>
    <h1>References</h1>
    <ol>
      <li id="ref-1">
        Pinkus, Allan. <em>Ridge functions.</em> Vol. 205. Cambridge University Press, 2015.
      </li>
      <li id="ref-2">
        Pinkus, Allan. <em>Approximation theory of the MLP model in neural networks.</em> Acta numerica 8 (1999): 143-195.
      </li>
      <li id="ref-3">
        Vostrecov, B. A., and M. A. Kreines. <em>Approximation of continuous functions by superpositions of plane waves.</em> Dokl. Akad. Nauk SSSR. Vol. 140. 1961.
      </li>
    </ol>
  </section>
  <section>
    <h1>Updates and Corrections</h1>
    <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/lucagrementieri/lucagrementieri.github.io">open an issue on GitHub</a>.</p>
  </section>
</footer>

			</article>
		</main>
		<footer>
  <span>
    &copy; <time datetime="2020-04-26 12:04:41 +0200">2020</time> Luca Grementieri. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme and
    <a href="https://github.com/nextbitlabs/Rapido">Rapido</a>.
  </span>
</footer>

	</body>
</html>
