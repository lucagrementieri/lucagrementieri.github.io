---
layout: post
title: Artificial neurons as ridge functions
description: Approximation theory of ridge functions
intro: Artificial neurons are a special case of ridge functions and linear combinations of ridge functions are a generalization of multilayer perceptrons with one hidden layer. The approximation theory of ridge functions is an important gateway to the study of approximation capabilities of neural networks.
---
<!-- improve intro -->
$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}
\gdef\a{\bold{a}}

\gdef\R{\mathbb{R}}
\gdef\Rn{\mathbb{R}^{n}}
\gdef\M{\mathcal{M}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
$
<section>
  <h1 id="approximation-theory">Approximation theory</h1>
  <p>
    Approximation theory is the branch of mathematics that studies how to approximate a class of functions
    with a set of simpler functions. It allows to discover set of functions described by finite or countable
    parameters that can faithfully represent an uncountable class of functions.
    This subject has great practical interest because it allows to solve optimization problems on functions without making strong assumptions on the searched function.
  </p>
  <p>
    Given a functional $\ell: \mathcal{F} \longrightarrow \R$,
    an optimization problem takes the form
    $$
    f^{*} = \argmin{f \in \mathcal{F}}\ \ell(f),
    $$
    where $\mathcal{F}$ is the search space of functions determined by the assumptions on $f^{*}$.
    For example, if we know that the solution $f^{*}$ is a continuous univariate function, then $\mathcal{F} = C(\R)$.
  </p>
  <p>
    In practice we cannot solve the optimization problem on a function space with an infinite number of dimensions,
    because machines can work only with a finite number of parameters. So we need to restrict $\mathcal{F}$ to a more
    manageable subset $\mathcal{G} \subset \mathcal{F}$.
    This restriction adds more assumptions on $f^{*}$, assumptions not dictated by the problem itself but by
    computational requirements. For example, in many cases, the search space is restricted to linear functions.
  </p>
  <p>
    Approximation theory looks for a countable or uncountable $\mathcal{G}$ dense in $\mathcal{F}$.
    The density property assures that the effective search space is not reduced because $f^{*}$ can be described
    by functions in $\mathcal{G}$ with arbitrary precision.
    Let us recall the definition of density.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $(X, d)$ be a metric space and let $S \subseteq X$. $S$ is <strong>dense</strong> in $X$
    if for every $x \in X$ and every $\epsilon > 0$, there exist $s \in S$ such that $d(x, s) < \epsilon$.
    Equivalently, $S$ is dense in $X$ if the closure of $S$ is $X$: $\overline{S} = X$.
  </p>
  <p>
    The most known result of approximation theory is Weierstrass Approximation Theorem.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f$ be a continuous real-valued function defined on the interval $[a, b]$.
    For every $\epsilon > 0$, there exists a polynomial $p$ such that $\forall\, x \in [a, b]$, we have
    $|f(x) − p(x)| < \epsilon$, or equivalently, the supremum norm $||f−p|| < \epsilon$.
  </p>
</section>
<section>
  <h1 id="mlp">Multilayer perceptron</h1>
  <p>
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of <em>layers</em>.
    Every layer is composed by <em>neurons</em>, the fundamental processing unit of the network.
    Every artificial neuron in the intermediate <em>hidden layers</em> processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the <em>activation function</em> $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    <small>
      In the neural network literature, the values $w_{i}$ are called <em>weights</em> and they model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron. $\theta$ is called <em>bias</em> and it recalls the threshold potential involved in the firing of biologic neurons.
    </small>
  </p>
  <p>
    In the final layer, said the <em>output layer</em>, neurons operates differently. They just perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this model because it can already approximate a very large class of functions.
    <!-- add small to explain that we are dealing with regression but classification is shaped as a regression too-->
  </p>
</section>
<section>
  <h1 id="ridge">Ridge functions</h1>
  <p>
    The function $N(\x)$ has a particular form: it is the composition of a univariate function $\sigma$ with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>ridge function</strong> is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  </p>
  <p>
    The vector $\a$ is called the <em>direction</em> because a ridge function is a multivariate function constant on the parallel hyperplanes orthogonal to $\a$. In fact, these hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  </p>
  <p>
    For a given direction $\a$, we denote the set of ridge functions with direction $\a$
    $$\M(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\M(\a) = \M(\bold{b})$.
  </p>
  <p>
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \M(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A <em>span</em> of a set $S$ is the set of linear combinations of elements of $S$, then
    every $F \in \M(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\M(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
    We are interested in the theory of approximation of sets $\M(\Omega)$ to better characterize the class of functions defined by MLPs.
  </p>
</section>
<section>
    <h1 id="operators">Operators</h1>
    <p>
        To extract information from a stream of data, we have to process it.
        Every operation applied to a signal is associated with an operator.
        For example, we can use operators to reduce noise, to recognize features,
        to detect peaks or discontinuities.
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
      A discrete operator $L$ is <strong>linear</strong> if $\forall\: a, b \in \mathbb{R}$
      $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
      A discrete operator is <strong>shift-invariant</strong> if $\forall\: p \in \mathbb{Z}$
      $$L(f[n-p]) = L(f)[n-p].$$
    </p>
    <p>
      When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant.
      The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters.
    </p>
    <p>
      This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series.
    </p>
</section>
<section>
    <h1 id="operators-convolution">Operators and convolution</h1>
    <p>
      There is a tight relationship between linear shift-invariant operators and convolution.
      To reveal this connection, we need to recall the discrete Dirac.
      Translating the discrete Dirac by $p$, we obtain
      $$
      \delta[n - p] =
      \begin{cases}
      1 & n = p \\
      0 & n \neq p
      \end{cases}.
      $$
      Using this expression, we can represent every signal as a linear combination of Dirac signals
      $$
      f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
      $$
      Now we have all the elements necessary to prove the main theorem of the article.
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    </p>
    <h3>Proof</h3>
    <p>
        If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$.
        Setting $h[n] = L(\delta)[n]$, we have
        $$
        \begin{aligned}
        L(f)[n] & = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) & \\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) & \textit{\footnotesize linearity} \\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] & \textit{\footnotesize shift-invariance}\\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &
        \end{aligned}
        $$
        To prove the inverse implication, we have to show that for every signal $h[n]$,
        $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution.
        Now, we show that the resulting operation is also shift-invariant.
        $$
        \begin{aligned}
            L(f[n-p]) & = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) & \\
            & = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) & \qquad {\footnotesize k' = k - p} \\
            & = L(f)[n - p]. & \square
        \end{aligned}
        $$
    </p>
    <p>
      This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations.
      We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons.
      Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines.
    </p>
    <p>
      Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too.
    </p>
</section>
<section>
  <h1 id="generalizations">Generalizations</h1>
  <p>
    In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g$ be two real function, the <strong>convolution</strong> of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt  \quad \forall\: x \in \mathbb{R}.
    $$
  </p>
  <p>
    All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions.
    <small>
      Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can.
    </small>
  </p>
  <p>
    Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension.
    We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals,
    the <strong>convolution</strong> of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k]  \quad \forall\: n \in \mathbb{Z}^{d}.
    $$
  </p>
</section>
<section>
    <h1 id="cnn">Convolutional Neural Networks</h1>
    <p>
      The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision.
      A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling.
    </p>
    <p>
      Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant.
    </p>
    <p>
      Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen.
    </p>
    <p>
      Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation!
      A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper "<a href=https://arxiv.org/abs/1904.11486>Making Convolutional Networks Shift-Invariant Again</a>".
    </p>
</section>
<section>
    <h1 id="conclusions">Conclusions</h1>
    <p>
      The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications.
    </p>
</section>
<footer style="text-align: left">
  <section>
    <h1>References</h1>
    <ol>
      <li id="ref-1">
        Mallat, Stéphane. <em>A wavelet tour of signal processing</em>. Elsevier, 1999.
      </li>
      <li id="ref-2">
        Zhang, Richard. <em>Making convolutional networks shift-invariant again</em>. arXiv preprint arXiv:1904.11486 (2019).
      </li>
    </ol>
  </section>
  <section>
    <h1>Updates and Corrections</h1>
    <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/lucagrementieri/lucagrementieri.github.io">open an issue on GitHub</a>.</p>
  </section>
</footer>
