---
layout: post
title: Artificial neurons as ridge functions
description: Approximation theory of ridge functions
intro: Artificial neurons are a special case of ridge functions and linear combinations of ridge functions are a generalization of multilayer perceptrons with one hidden layer. The approximation theory of ridge functions provides an upper bound on the approximation capabilities of neural networks.
---
$
\gdef\x{\bold{x}}
\gdef\w{\bold{w}}
\gdef\a{\bold{a}}

\gdef\R{\mathbb{R}}
\gdef\RR{\mathcal{R}}
\gdef\Rn{\mathbb{R}^{n}}
\gdef\M{\mathcal{M}}

\gdef\argmin#1{\underset{#1}{\text{arg\,min}}}
\gdef\max#1{\underset{#1}{\text{max}}}
$
<section>
  <h1 id="approximation-theory">Approximation theory</h1>
  <p>
    Approximation theory is the branch of mathematics that studies how to approximate a class of functions
    with a set of simpler functions. It allows to discover set of functions described by finite or countable
    parameters that can faithfully represent an uncountable class of functions.
    This subject has great practical interest because it allows to solve optimization problems on functions without making strong assumptions on the searched function.
  </p>
  <p>
    Given a functional $\ell: \mathcal{F} \longrightarrow \R$,
    an optimization problem takes the form
    $$
    f^{*} = \argmin{f \in \mathcal{F}}\ \ell(f),
    $$
    where $\mathcal{F}$ is the search space of functions determined by the assumptions on $f^{*}$.
    For example, if we know that the solution $f^{*}$ is a continuous univariate function, then $\mathcal{F} = C(\R)$.
  </p>
  <p>
    In practice we cannot solve the optimization problem on a function space with an infinite number of dimensions,
    because machines can work only with a finite number of parameters. So we need to restrict $\mathcal{F}$ to a more
    manageable subset $\mathcal{G} \subset \mathcal{F}$.
    This restriction adds more assumptions on $f^{*}$, assumptions not dictated by the problem itself but by
    computational requirements. For example, in many cases, the search space is restricted to linear functions.
  </p>
  <p>
    Approximation theory looks for a countable or uncountable $\mathcal{G}$ dense in $\mathcal{F}$.
    The density assures that the effective search space is not reduced because $f^{*}$ can be described
    by functions in $\mathcal{G}$ with arbitrary precision.
    Let us recall the definition of a dense set.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $X$ be a topological space and let $A \subseteq X$. $A$ is <strong>dense</strong> in $X$
    if for every $x \in X$, any neighborhood of $x$ contains at least one point from $A$.
    Equivalently, $A$ is dense in $X$ if the closure of $A$ is $X$: $\overline{A} = X$.
    <small>
      The definitions of neighborhood and closure comes from general topology.
      A neighborhood of a point $x$ is a set that includes an open subset that contains $x$.
      The closure of a set $S$ is the smallest closed set including $S$; it is the union of $S$
      and its limit points.
    </small>
  </p>
  <p>
    The most known result of approximation theory is <strong>Weierstrass Approximation Theorem</strong>.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f$ be a continuous real-valued function defined on the interval $[a, b]$.
    For every $\epsilon > 0$, there exists a polynomial $p$ such that $\forall\, x \in [a, b]$, we have
    $|f(x) − p(x)| < \epsilon$, or equivalently, $||f−p||_{\infty} < \epsilon$.
    <small>
      The notation $||\cdot||_{\infty}$ indicates the uniform or supremum norm.
      The norm assigns to a bounded function $f$ on the interval $[a, b]$ the value
      $||f||_{\infty} = \text{sup}\{|f(x)|, x \in [a, b]\}$.
    </small>
  </p>
</section>
<section>
  <h1 id="mlp">Multilayer perceptron</h1>
  <p>
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of <em>layers</em>.
    Every layer is composed by <em>neurons</em>, the fundamental processing unit of the network.
    Every artificial neuron in the intermediate <em>hidden layers</em> processes the outputs of the previous layer $\x$ with a non-linear function $N: \Rn \longrightarrow \R$ such that
    $$
    N(\x) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\w \cdot \x - \theta),
    $$
    where $\w$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the <em>activation function</em> $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    <small>
      In the neural network literature, the values $w_{i}$ are called <em>weights</em> and they model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron. $\theta$ is called <em>bias</em> and it recalls the threshold potential involved in the firing of biologic neurons.
    </small>
  </p>
  <p>
    In the final layer, said the <em>output layer</em>, neurons operates differently. They just perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\x) = \sum_{i=1}^{r} c_i \sigma\left(\w^i \cdot \x - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this model because it can already approximate a very large class of functions.
    <small>
      The MLPs model under consideration outputs a single real number.
      The analysis can be generalized to outputs in $\R^{m}$ by repeating it on every component.
      This allows to tackle regression problems with multiple outputs and in particular classification tasks.
      In fact, classification tasks are framed as regression tasks having class probabilities as targets.
    </small>
  </p>
</section>
<section>
  <h1 id="ridge">Ridge functions</h1>
  <p>
    The function $N(\x)$ has a particular form: it is the composition of a univariate function $\sigma$ with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>ridge function</strong> is a function $F: \Rn \longrightarrow \R$
    of the form
    $$
    F(\x) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\a \cdot \x),
    $$
    where $f: \R \longrightarrow \R$ is a univariate function and
    $\a = (a_1, \mathellipsis, a_n) \in \Rn - \{\bold{0}\}$ is a fixed vector.
  </p>
  <p>
    The vector $\a$ is called the <em>direction</em> because a ridge function is a multivariate function constant on the parallel hyperplanes orthogonal to $\a$. In fact, these hyperplanes are defined by the equation $\a \cdot \x = c$, with $c \in \R$.
  </p>
  <p>
    For a given direction $\a$, we denote the set of ridge functions with direction $\a$
    $$\RR(\a) = \{f(\a \cdot \x), f: \R \longrightarrow \R\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\a = \lambda \bold{b}$ for some $\lambda \in \R - \{0\}$, then $\RR(\a) = \RR(\bold{b})$.
  </p>
  <p>
    For a set $\Omega \subseteq \Rn$, we define
    $$
    \RR(\Omega) = \text{span}\{f(\a \cdot \x), f: \R \longrightarrow \R, \a \in \Omega\}.
    $$
    A <em>span</em> of a set $S$ is the set of linear combinations of elements of $S$, then
    every $F \in \RR(\Omega)$ has the form
    $$
    F(\x) = \sum_{i=1}^{r} c_i f_i(\a^i \cdot \x).
    $$
    We can notice that $\RR(\Rn)$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
  </p>
  <p>
    If we require the continuity of the activation function $\sigma$ in a MLP, the functions defined by a MLP
    belong to the smaller set of linear combinations of continuous ridge functions
    $$
    \M(\Omega) = \text{span}\{f(\a \cdot \x), f \in C(\R), \a \in \Omega\}
    $$
    with $\Omega = \Rn$.
  </p>
  <p>
    We are interested in the theory of approximation of sets $\M(\Omega)$
    to better characterize the class of functions defined by a MLP.
    Functions well approximated by a MLPs are a subset of the functions
    approximated by $\M(\Rn)$, therefore $\M(\Rn)$ gives us an upper bound on
    the approximation capabilities of a MLP.
  </p>
</section>
<section>
  <h1 id="compact-convergence">Uniform convergence on compact sets</h1>
  <p>
    $\M(\Omega)$ is a large class of continuous functions. We are going to find the conditions
    on $\Omega$ that assure that it is dense in the set of continuous functions $C(\Rn)$.
    Since density is a property of a topological space, we have to define a topology
    on $C(\Rn)$ to clarify the meaning of this statement.
    Approximation theory of continuous functions commonly use the topology of uniform
    converge on compact subsets.
    <small>
      The general topological definition of a compact set involve covers and subcover.
      The Heine-Borel theorem gives a much simpler characterization of compactness in an euclidean space:
      in $\Rn$ a set is compact if and only if it is closed and bounded.
    </small>
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A sequence of functions $f_{m} \in C(\Rn), m \in \mathbb{N}$ is said to
    <strong>converge uniformly on compact sets</strong> as $m \to \infty$ to some function
    $f \in C(\Rn)$ if, for every compact set $K \subset \Rn$ and every $\epsilon > 0$,
    there exist $\overline{m}$ such that $\forall\, m \geq \overline{m}$
    $$
    ||f_{m} - f||_{K} = \max{x \in K}\ | f_{m}(x) - f(x)| < \epsilon.
    $$
    <small>
      If we can prove density in this topology, then the subset is also dense in many other topologies.
      For example, density will hold in $L^{p}(K)$, where $K$ is any compact subset of $\Rn$ and $p \in [1,\infty[$.
      Indeed $||f||_{p,K} \leq ||f||_{K} < +\infty$.
    </small>
  </p>
  <p>
    A very powerful tool to prove results with this topology is the <strong>Stone-Weierstrass Theorem</strong>.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Suppose $X$ is a compact Hausdorff space and $A$ is a subalgebra of $C(X)$, the space of continuous real-valued functions over $X$, which contains a non-zero constant function. Then $A$ is dense in $C(X)$ if and only if it separates points.
    <small>
      This theorem implies Weierstrass Approximation Theorem since the polynomials on $[a, b]$ form a subalgebra of $C([a, b])$ which contains the constants and separates points.
    </small>
  </p>
  <p>
    Let us recall the definition of algebra and subalgebra.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $A$ be a vector space over $\R$ equipped with a binary operation (a product) from $\cdot: A \times A \to A$. Then $A$ is an <strong>algebra</strong> if the following identities hold for all elements $x$, $y$, and $z$ of $A$, and all scalars $a$ and $b$ of $\R$:
    $$
    \begin{aligned}
    (x + y) \cdot z &= x \cdot z + y \cdot z; & \qquad \textit{\footnotesize right distributivity}\\
    z \cdot (x + y) &= z \cdot x + z \cdot y; & \qquad \textit{\footnotesize left distributivity}\\
    (ax) \cdot (by) &= (ab) (x \cdot y). & \qquad \textit{\footnotesize compatibility with scalars}
    \end{aligned}
    $$
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>subalgebra</strong> is a subset of an algebra, closed under addition, multiplication by scalar and
    product.
  </p>
</section>
<section>
  <h1 id="compact-convergence">Density of continuous ridge functions</h1>
  <p>
    We now have all the ingredient to state a density theorem about $\M(\Rn)$ in $C(\Rn)$.
    In reality the theorem is stronger because it proves that $\M(\mathbb{Z}^{n})$ is dense in $C(\Rn)$.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    $\M(\Rn)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets.
  </p>
  <h3>Proof</h3>
  <p>
    It is sufficient to prove that linear combinations of the functions $e^{\bold{n} \cdot \x}$,
    where $\bold{n} \in \mathbb{Z}^{n}$ are dense in $C(\Rn)$, in the topology of uniform convergence on compact subsets. This fact is a consequence of the Stone-Weierstrass Theorem.
  </p>
  <p>
    For every compact set $K \subset \Rn$, $C(K)$ is an algebra: it is a vector space and the product between functions satisfies all the properties
    listed in the definition of algebra.
    The set $\mathcal{E}_{K} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}, x \in K \}$ is a linear subspace of $C(K)$, therefore it is closed under addition and multiplication by a scalar.
    Thanks to the properties of the exponential function, the product of two elements $f, g, \in \mathcal{E}_{K}$ is still included in $\mathcal{E}_{K}$:
    $$
    f = \sum_{i=1}^s e^{\bold{n}_i \cdot \x} \quad g = \sum_{j=1}^t e^{\bold{m}_j \cdot \x} \Rightarrow f \cdot g = \sum_{i=1}^s \sum_{j=1}^t e^{(\bold{n}_i + \bold{m}_j) \cdot x}.
    $$
    Thus, $\mathcal{E}_{K}$ is a subalgebra of $C(K)$. Putting $\bold{n} = \bold{0}$, we obtain
    the non-zero constant function $e^{\bold{0} \cdot \x} = 1$.
    Moreover $\mathcal{E}_{K}$ separates points. Let $\bold{y} \neq \bold{z} \in \Rn$, then
    there exists a coordinate $h \in \{1, \mathellipsis, n\}$ such that $y_h \neq z_h$.
    The function $e^{\bold{e}_h \cdot \x}$, where $\bold{e}_h = (0, \mathellipsis, 0, 1, 0, \mathellipsis, 0)$
    is $h$-th vector of the canonical basis, separates $\bold{y}$ from $\bold{z}$ since
    $$e^{\bold{e}_h \cdot y} = e^{y_h} \neq e^{z_h} = e^{\bold{e}_h \cdot z}.$$
    All the hypotheses of Stone-Weierstrass theorem are satisfied, so for every compact $K$, $\mathcal{E}_{K}$
    is dense in $C(K)$ with respect to the uniform norm. That is equivalent to say that $\mathcal{E} = \text{span}\{e^{\bold{n} \cdot \x}, \bold{n} \in \mathbb{Z}^{n}\}$ is dense in $C(\Rn)$ in the topology of
    uniform convergence on compact subsets.
    <span style="float:right">$\square$</span>
  </p>
  <p>
    The density of $\M(\Omega)$ in $C(\Rn)$ tells us that for every continuous functions $f$ it exists a sequence
    of functions in $\M(\Omega)$ whose limit is $f$ in the chosen topology.
    Thus, any continuous function can be approximated by functions in $\M(\Omega)$ with an arbitrary small
    approximation error.
    <small>
      The uniform convergence on compact sets is not equivalent to uniform convergence.
      In practice, it does not matter because we always approximate functions whose domain can be included in a compact set; in fact, the function to be approximated is often known only on a finite set of points.      
    </small>
  </p>
  <p>
    Since $\M(\Omega)$ only depends on the set of normalized directions in $\Omega$, the
    theorem also implies that $\M([-M, M]^n)$ is dense in $C(\Rn)$ for an arbitrary constant $M$.
    In general, we can safely reduce the space of directions of ridge functions,
    for example limiting their norm, without losing expressive power.
  </p>
  <p>
    A more general theorem by Vostrecov and Kreines states a necessary and sufficient condition
    for the density of $\M(\Omega)$ in $C(\Rn)$.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    $\M(\Omega)$ is dense in $C(\Rn)$ in the topology of uniform convergence on compact subsets,
    if and only if there is no homogeneous polynomial $p \neq 0$ such that $p(\x) = 0 \ \forall\, \x \in \Omega$.
  </p>
</section>
<section>
    <h1 id="conclusions">Conclusions</h1>
    <p>
      Single hidden layer MLPs are a special case of a linear combination of ridge functions.
      We have proved that the class of functions spanned by continuous ridge function can approximate
      every continuous function with arbitrary precision.
      This fact is relevant in applications because it allows to design parametric models without imposing
      compelling constraints.
    </p>
    <p>
      This density result does not imply that the same property holds true for MLPs, but
      we can hope that functions represented by a MLP have the same approximation capability (indeed they have).
    </p>
</section>
<footer style="text-align: left">
  <section>
    <h1>References</h1>
    <ol>
      <li id="ref-1">
        Pinkus, Allan. <em>Ridge functions.</em> Vol. 205. Cambridge University Press, 2015.
      </li>
      <li id="ref-2">
        Pinkus, Allan. <em>Approximation theory of the MLP model in neural networks.</em> Acta numerica 8 (1999): 143-195.
      </li>
      <li id="ref-3">
        Vostrecov, B. A., and M. A. Kreines. <em>Approximation of continuous functions by superpositions of plane waves.</em> Dokl. Akad. Nauk SSSR. Vol. 140. 1961.
      </li>
    </ol>
  </section>
  <section>
    <h1>Updates and Corrections</h1>
    <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/lucagrementieri/lucagrementieri.github.io">open an issue on GitHub</a>.</p>
  </section>
</footer>
